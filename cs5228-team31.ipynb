{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import probplot\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# basic\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "import pylab\n",
    "import category_encoders as ce\n",
    "\n",
    "# preprocessomg\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, ParameterGrid, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import RidgeCV, ElasticNetCV, LassoCV, Lasso, ElasticNet, Ridge\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "# compress warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data\n",
    "train_data_path = './dataset/train.csv'\n",
    "test_data_path = './dataset/test.csv'\n",
    "\n",
    "# auxiliary data\n",
    "mrt_exist_data_path = './dataset/auxiliary-data/sg-mrt-existing-stations.csv'\n",
    "mrt_planned_data_path = './dataset/auxiliary-data/sg-mrt-planned-stations.csv'\n",
    "mall_data_path = './dataset/auxiliary-data/sg-shopping-malls.csv'\n",
    "primary_school_data_path = './dataset/auxiliary-data/sg-primary-schools.csv'\n",
    "coe_price_data_path = './dataset/auxiliary-data/sg-coe-prices.csv'\n",
    "stock_price_data_path = './dataset/auxiliary-data/sg-stock-prices.csv'\n",
    "\n",
    "# Load Data\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_data['monthly_rent'] = 0\n",
    "mrt_exist_data = pd.read_csv(mrt_exist_data_path)\n",
    "mrt_planned_data = pd.read_csv(mrt_planned_data_path)\n",
    "mall_data = pd.read_csv(mall_data_path)\n",
    "primary_school_data = pd.read_csv(primary_school_data_path)\n",
    "coe_price_data = pd.read_csv(coe_price_data_path)\n",
    "stock_price_data = pd.read_csv(stock_price_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 新加坡的住宅物业: 二房式灵活组屋（2-room flexi flat）、三房式组屋（3-room flat）、四房式组屋（4-room flat）、五房式组屋（5-room flat）、三代同堂组屋（3Gen flat）、公寓式组屋（Executive flat）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_type\n",
    "train_data['flat_type'] = train_data['flat_type'].str.replace(\"-\", \" \")\n",
    "test_data['flat_type'] = test_data['flat_type'].str.replace(\"-\", \" \")\n",
    "\n",
    "# monthly_rent\n",
    "mean = train_data['monthly_rent'].mean()\n",
    "std = train_data['monthly_rent'].std()\n",
    "std_multiplier = 3\n",
    "lower_threshold = mean - std_multiplier * std\n",
    "upper_threshold = mean + std_multiplier * std\n",
    "train_data = train_data[(train_data['monthly_rent'] >= lower_threshold) & (train_data['monthly_rent'] <= upper_threshold)]\n",
    "\n",
    "# mrt_planned\n",
    "mrt_planned_data['opening_year'].replace('TBA', pd.NA, inplace=True)\n",
    "mrt_planned_data.dropna(subset=['opening_year'], inplace=True)\n",
    "# only include those planned within 3 years, due to the policy restriction\n",
    "mrt_planned_data = mrt_planned_data[(mrt_planned_data['opening_year']).astype(int) < 2029]\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA - Main Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 General Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Visualization of General Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data,height=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "There is no null value for any column in the dataset.\n",
    "The scale of numerical data varies significantly, remember to normalize before analysis\n",
    "There are natural groups but not clear, further observation required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Univariate Analysis - Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 a) town & subarea"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_counts = train_data['town'].value_counts()\n",
    "planning_area_counts = train_data['planning_area'].value_counts()\n",
    "\n",
    "# Merge the two Series into a single DataFrame\n",
    "combined_df = pd.DataFrame({'Town': town_counts, 'Planning Area': planning_area_counts}).fillna(0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Width of a bar\n",
    "width = 0.4\n",
    "\n",
    "# Positions of bars on x axis\n",
    "r1 = range(len(combined_df))\n",
    "r2 = [x + width for x in r1]\n",
    "\n",
    "# Creating bars\n",
    "plt.bar(r1, combined_df['Town'], width=width, label='Town', color='blue', edgecolor='grey')\n",
    "plt.bar(r2, combined_df['Planning Area'], width=width, label='Planning Area', color='red', edgecolor='grey')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Comparing Town and Planning Area Counts', fontweight='bold')\n",
    "\n",
    "# X axis\n",
    "plt.xlabel('Areas', fontweight='bold')\n",
    "plt.xticks([r + width for r in range(len(combined_df))], combined_df.index, rotation=90)\n",
    "\n",
    "# Y axis\n",
    "plt.ylabel('Counts', fontweight='bold')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 b) flat_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['flat_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 c) flat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_model_counts = train_data['flat_model'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15,6))  # Adjust the figure size\n",
    "sns.barplot(y=flat_model_counts.index, x=flat_model_counts.values, palette='viridis')  # Use y for town names to get a horizontal bar plot\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Number of Occurrences by Flat Model')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Flat Model')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 d) furnished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['furnished'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 e) latitude & longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latitude = train_data['latitude']\n",
    "longitude = train_data['longitude']\n",
    "\n",
    "avg_lat, avg_lon = latitude.mean(), longitude.mean()\n",
    "\n",
    "# Create a base map\n",
    "m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)\n",
    "\n",
    "# Add the heat map\n",
    "heat_data = [[lat, lon] for lat, lon in zip(train_data['latitude'], train_data['longitude'])]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "\n",
    "# Display the map if NEEDED\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 f) subzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['subzone'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 g) region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 h) block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['block'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Univariate Analysis - Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 a) monthly_rent statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(20, 12))\n",
    "ax1 = sns.histplot(x = train_data['monthly_rent'], color='teal', ax= ax[0, 0])\n",
    "ax2 = sns.boxplot(x = train_data['monthly_rent'], ax= ax[0, 1], color= 'teal')\n",
    "ax3 = sns.violinplot(x = train_data['monthly_rent'], ax= ax[1, 0], color= 'teal')\n",
    "ax4 = probplot(train_data['monthly_rent'], plot=pylab)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 b) region v.s. monthly_rent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale = 1.3)\n",
    "plt.figure(figsize=(15, 4))\n",
    "ax = sns.barplot(x=train_data['region'],\n",
    "                 y=train_data['monthly_rent'],\n",
    "                 palette='viridis',\n",
    "                 ci = None)\n",
    "plt.ylabel('Monthly Rent');\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x() + 0.4, p.get_height() + 1), ha = 'center', va = 'bottom', color = 'Black')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 c) flat_type v.s. monthly_rent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale = 1.3)\n",
    "plt.figure(figsize=(15, 4))\n",
    "ax = sns.barplot(x=train_data['flat_type'],\n",
    "                 y=train_data['monthly_rent'],\n",
    "                 palette='viridis',\n",
    "                 ci = None)\n",
    "plt.ylabel('Monthly Rent');\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x() + 0.4, p.get_height() + 1), ha = 'center', va = 'bottom', color = 'Black')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 d) flat_model v.s. monthly_rent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale = 1.3)\n",
    "plt.figure(figsize=(15, 4))\n",
    "ax = sns.barplot(x=train_data['flat_model'],\n",
    "                 y=train_data['monthly_rent'],\n",
    "                 palette='viridis',\n",
    "                 ci = None)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "plt.ylabel('Monthly Rent');\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x() + 0.4, p.get_height() + 1), ha = 'center', va = 'bottom', color = 'Black')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 e) lease_commence_date v.s. monthly_rent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.sort_values('lease_commence_date')\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(16, 4))\n",
    "sns.lineplot(x='lease_commence_date', y='monthly_rent', data=train_data, ci=None)\n",
    "\n",
    "plt.title('Trend of Monthly Rent Over Years')\n",
    "plt.xlabel('Year of Lease Commencement')\n",
    "plt.ylabel('Average Monthly Rent')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.5 e) floor_area_sqm v.s. monthly_rent"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "sns.histplot(train_data['floor_area_sqm'], bins=30, kde=False, color='orange', ax=ax[0])  # Adjust position as desired\n",
    "sns.scatterplot(x=train_data['floor_area_sqm'], y=train_data['monthly_rent'], ax=ax[1], color='green', alpha=0.5)  # Adjust position as desired\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 EDA - Aux Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 a) coe\n",
    "* To register a vehicle, you must first bid for a COE.Category refers to different vehicle types. Some background info: https://onemotoring.lta.gov.sg/content/onemotoring/home/buying/upfront-vehicle-costs/certificate-of-entitlement--coe-.html\n",
    "1. category: Different categories might represent different types of vehicles, such as motorcycles, passenger cars, commercial vehicles, etc.\n",
    "2. month: the month in which the COE bid took place.\n",
    "3. bidding: the bidding round or session within the specified month.\n",
    "4. price: the successful bid price for the COE in that category and bidding session. It represents the cost to obtain the COE.\n",
    "5. quota: the number of COEs available for bidding in that category and session. It's the supply side of the equation.\n",
    "6. bids: the number of bids received for the available COEs in that category and session. It's the demand side of the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coe_price_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 b) primary_school\n",
    "1. singapore primary school choose: https://investingsg.com/keyinfo/schools-properties/. Key points:\n",
    "* 什么样的房子算学区房？\n",
    "    * 教育部说了 ”每一所小学都是好学校“。但是，民间排名一直都在。一般意义上是在名校一公里范围内，抽签概率好的学校可以扩大到两公里范围。如果做社区义工还要注意住址和小学是否在同一个GRC。\n",
    "* 租房租多久，什么时候开始租？\n",
    "    * 报名是按IC地址，所以没有明确开始租房日期，只要报名时IC地址改了就可以。报名成功后需住满30个月，这项规定同样适用于买的房子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_school_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 c) mall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 d) mrt_planned\n",
    "* According to a friend in the Ministry:\n",
    "    * 0.3 KM: MRT-house\n",
    "    * 1KM: MRT-near-house\n",
    "* https://www.propertyguru.com.sg/property-guides/mrt-effect-on-property-prices-39498"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrt_planned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 e) mrt_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrt_exist_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 f) stock_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_data['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Observation - Main Data\n",
    "1. monthly rent\n",
    "* it's not heavily skewed, but there are definately outliers\n",
    "* regions - the average rent across different regions are roughly the same\n",
    "* flat_type - the average rent is positively correlated to the #room\n",
    "* flat_model - those with extremely low frequency in flat_model usually have a high price\n",
    "* lease_commence_date - graph indicates the rent is increasing yearly, and exhibit up and downs in each decade. this can be combined with the auxiliary data to analyze the influence of economics\n",
    "* floor_area_sqm - generally normal distribution, but obvious outliers\n",
    "* latitude&longitude - observe from a higher angle, there are definately groups\n",
    "2. region\n",
    "* the rent data across regions varies less than 1%, this feature is providing very limited infomation\n",
    "\n",
    "3. town & planning_area\n",
    "* these two features' value overlaps, and there are 3 extra values in planning_area. binary encoded with other geographical features?\n",
    "4. flat_type\n",
    "* 4/3/5 rooms are most popular, does that mean sufficient demand or supply? (may influence price)\n",
    "5. furnished\n",
    "* 100% furnished, can we drop this feature?\n",
    "6. latitude & longitude\n",
    "* there are actually naturally formed groups, how to group them properly?\n",
    "7. subzone\n",
    "* maybe can be used with the latitude * longitude?\n",
    "8. block\n",
    "* What is block_NO: https://www.quora.com/Why-did-Singapore-start-having-HDB-block-numbers-with-letters-e-g-172A-B-C\n",
    "* To sum up: each number is a location, and the following letter is the age. However, we have location & lease date info, thus drop this"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## 1.9 Observation - Aux Data\n",
    "1. coe_price_data\n",
    "* figure out whether the COE is a lagging or not. More specifically, different types of COE may vary. Main focus should be on Category_C: Goods vehicle&bus, and Category_D: Motorcycle v.s. Category_A and Category_B. The former are used as productivity materials, while the latter is consumption.\n",
    "2. primary_school_data\n",
    "* for each school, find its KNN house, and statistics w.r.t rent: 1km_school_rent_mean, 1km_school_rent_median, 2km_school_rent_mode, ...;\n",
    "* for each record, find all primary_schools within 1km, 2km range, and average out the statistics before-mentioned.\n",
    "3. mall\n",
    "* for each mall, find its KNN house, and the statistics: mall_average_rent, mall_median_rent, mall_75_rent, mall_25_rend;\n",
    "* for each record, find the malls within 3km range, and average out the statistics before-mentioned.\n",
    "4. mrt_planned\n",
    "* for each record, find the #mrt_planned within 0.3KM and 1KM, with those beyond 3 years ones ignored.\n",
    "5. mrt_exist\n",
    "* for each record, find the #mrt_planned within 0.3KM and 1KM.\n",
    "6. stock_price\n",
    "* stock_price is a leading indicator of economics, while rent_price is lagging indicator. This relationship implies that stock_price can be used to predict the overall rent price.\n",
    "* stock_price includes 59 stocks, thus we can form an index, and compute the index value on each trading day. However, those extremely illiquid stocks should be removed.\n",
    "* Time-Series-Regression can have some weight in the final model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Functions\n",
    "def encode_normalization_with_scaler(data: pd.DataFrame, column_name: str, scaler, keep_origin=False):\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler()\n",
    "        scaler.fit(data[[column_name]])\n",
    "\n",
    "    new_column_name = column_name\n",
    "    if keep_origin:\n",
    "        new_column_name = column_name + '_norm'\n",
    "\n",
    "    data[new_column_name] = scaler.transform(data[[column_name]])\n",
    "    return data, scaler\n",
    "\n",
    "def encode_dict_map(data: pd.DataFrame, column_name: str, transform_map: dict):\n",
    "    data[column_name] = data[column_name].map(transform_map)\n",
    "    return data\n",
    "\n",
    "def encode_drop(data: pd.DataFrame, column_name: str):\n",
    "    data.drop(columns=[column_name], inplace=True)\n",
    "    return data\n",
    "\n",
    "# 3. block: drop this feature, refer to EDA observations\n",
    "train_data = encode_drop(train_data, 'block')\n",
    "test_data = encode_drop(test_data, 'block')\n",
    "\n",
    "# 5. flat type\n",
    "# there is a ranking in features, thus label-mapping\n",
    "flat_type_mapping_dict = {'2 room': 2, '3 room': 3, '4 room': 4, '5 room': 5, 'executive': 6}\n",
    "train_data = encode_dict_map(train_data, 'flat_type', flat_type_mapping_dict)\n",
    "test_data = encode_dict_map(test_data, 'flat_type', flat_type_mapping_dict)\n",
    "\n",
    "# # 7. floor area sqm\n",
    "# # use it directly or use it after normalization.\n",
    "# scaler = MinMaxScaler().fit(train_data[['floor_area_sqm']])\n",
    "# train_data, scaler = encode_normalization_with_scaler(train_data, 'floor_area_sqm', scaler)\n",
    "# test_data, _ = encode_normalization_with_scaler(test_data, 'floor_area_sqm', scaler)\n",
    "\n",
    "# 8. furnished\n",
    "# drop attributes: the value of this attribute for all data is \"yes\"\n",
    "train_data = encode_drop(train_data, 'furnished')\n",
    "test_data = encode_drop(test_data, 'furnished')\n",
    "\n",
    "# 11. elevation - Similar to attribute \"furnished\"\n",
    "train_data = encode_drop(train_data, 'elevation')\n",
    "test_data = encode_drop(test_data, 'elevation')\n",
    "\n",
    "# 14.region: drop this feature, refer to EDA observations\n",
    "train_data = encode_drop(train_data, 'region')\n",
    "test_data = encode_drop(test_data, 'region')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering\n",
    "1. Binning of numerical variables - https://www.kaggle.com/competitions/two-sigma-connect-rental-listing-inquiries/discussion/32116\n",
    "2. create new features by encoding categorical variables using statistics (like mean, median, standard deviation, etc.) of continuous variables. https://www.kaggle.com/competitions/two-sigma-connect-rental-listing-inquiries/discussion/32123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create New Features after EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.1 a) Date Related Features\n",
    "1. lease_commence_date -> real_estate_age\n",
    "2. rent_approval_date -> rent_year, rent_month"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # lease_commence_date -> real_estate_age\n",
    "    dataset['real_estate_age'] = 2023 - dataset['lease_commence_date']\n",
    "    dataset = encode_drop(dataset, 'lease_commence_date')\n",
    "\n",
    "    # rent_approval_date -> rent_year, rent_month\n",
    "    split_date = dataset['rent_approval_date'].str.split('-')\n",
    "    dataset['rent_year'] = split_date.str[0].astype('int32')\n",
    "    dataset['rent_month'] = split_date.str[1].astype('int32')\n",
    "    dataset = encode_drop(dataset, 'rent_approval_date')\n",
    "\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Append Aux-Data to Train and Test df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 a) Stock Price\n",
    "1. remove all illiquid stocks\n",
    "2. compute the monthly index price and volatility based on adjusted_close_price\n",
    "3. concat to train/test where rent_month = month, rent_year = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove illiquid stocks\n",
    "mask = stock_price_data['name'].transform(lambda x: stock_price_data['name'].value_counts()[x] >= 100)\n",
    "liquid_stock_price_data = stock_price_data[mask]\n",
    "\n",
    "# Convert the \"date\" column to datetime format\n",
    "liquid_stock_price_data['date'] = pd.to_datetime(liquid_stock_price_data['date'])\n",
    "\n",
    "# Extract year and month\n",
    "liquid_stock_price_data['year'] = liquid_stock_price_data['date'].dt.year\n",
    "liquid_stock_price_data['month'] = liquid_stock_price_data['date'].dt.month\n",
    "\n",
    "# Group by year and month, then calculate mean and standard deviation\n",
    "monthly_stats = liquid_stock_price_data.groupby(['year', 'month'])['adjusted_close'].agg(\n",
    "    index_price='mean',\n",
    "    index_volatility='std'\n",
    ").reset_index()\n",
    "\n",
    "datasets = [train_data, test_data]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # Merge train_data with monthly_stats\n",
    "    dataset = dataset.merge(\n",
    "        monthly_stats,\n",
    "        left_on=['rent_year', 'rent_month'],\n",
    "        right_on=['year', 'month'],\n",
    "        how='left'\n",
    "    )\n",
    "    # Drop the 'year' and 'month' columns from merged_data as they are redundant\n",
    "    dataset.drop(columns=['year', 'month'], inplace=True)\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 b) Primary School, Mrt, and Mall\n",
    "* for each record of each aux-data: find its KNN houses and calculate statistics\n",
    "* for each house, find its KNN aux-record, and weighted-average their statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg(values, weights):\n",
    "    \"\"\"Compute the weighted average of values.\"\"\"\n",
    "    return np.dot(values, weights) / weights.sum()\n",
    "\n",
    "def get_knn_statistics(base_data, aux_data, k):\n",
    "    \"\"\"Compute KNN statistics for given datasets.\"\"\"\n",
    "\n",
    "    # Convert latitude and longitude to radians for haversine metric\n",
    "    base_data_rad = np.radians(base_data[['latitude', 'longitude']])\n",
    "    aux_data_rad = np.radians(aux_data[['latitude', 'longitude']])\n",
    "\n",
    "    # Initialize KNN\n",
    "    knn = NearestNeighbors(n_neighbors=k, algorithm='ball_tree', metric='haversine')\n",
    "\n",
    "    # Fit and find KNN houses for each record in aux_data\n",
    "    knn.fit(base_data_rad)\n",
    "    distances, indices = knn.kneighbors(aux_data_rad)\n",
    "    distances += 1e-10\n",
    "\n",
    "    aux_data['median_rent'] = [base_data.iloc[ind]['floor_area_sqm'].median() for ind in indices]\n",
    "    aux_data['std_rent'] = [base_data.iloc[ind]['floor_area_sqm'].std() for ind in indices]\n",
    "    aux_data['weighted_mean_rent'] = [weighted_avg(base_data.iloc[ind]['floor_area_sqm'], 1/dist) for ind, dist in zip(indices, distances)]\n",
    "\n",
    "    # Fit and find KNN aux-records for each rent in the base_data\n",
    "    knn = NearestNeighbors(n_neighbors=5)\n",
    "    knn.fit(aux_data[['latitude', 'longitude']])\n",
    "    distances, indices = knn.kneighbors(base_data[['latitude', 'longitude']])\n",
    "\n",
    "    stats = {\n",
    "        'weighted_mean_distance': [weighted_avg(aux_data.iloc[ind]['weighted_mean_rent'], 1/dist) for ind, dist in zip(indices, distances)],\n",
    "        'median_distance': [weighted_avg(aux_data.iloc[ind]['median_rent'], 1/dist) for ind, dist in zip(indices, distances)],\n",
    "        'std_distance': [weighted_avg(aux_data.iloc[ind]['std_rent'], 1/dist) for ind, dist in zip(indices, distances)]\n",
    "    }\n",
    "\n",
    "    return stats\n",
    "\n",
    "# Process datasets\n",
    "datasets = [train_data, test_data]\n",
    "aux_datasets = {\"school\": primary_school_data,\n",
    "                \"mrt_plan\": mrt_exist_data,\n",
    "                \"mrt_exist\": mrt_exist_data,\n",
    "                \"mall\": mall_data}\n",
    "k_values = {\"school\": 20,\n",
    "                \"mrt_plan\": 5,\n",
    "                \"mrt_exist\": 5,\n",
    "                \"mall\": 20}\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    for key, aux_df in aux_datasets.items():\n",
    "        k = k_values.get(key)\n",
    "        stats = get_knn_statistics(dataset, aux_df, k)\n",
    "\n",
    "        dataset[f'{key}_median_distance'] = stats['median_distance']\n",
    "        dataset[f'{key}_std_distance'] = stats['std_distance']\n",
    "        dataset[f'{key}_weighted_mean_distance'] = stats['weighted_mean_distance']\n",
    "\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 c) Lifecycle Facility Counts\n",
    "* compute the trans-cycles according to the gov plan\n",
    "    * MRT: 0.3 KM  MRT-house, 1KM: MRT-near-house\n",
    "    * Primary School: not explicit data, assume the 15min driving cycle of a school bus, 5KM\n",
    "    * Mall: 15min driving cycle, approximately 8KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def count_nearby_records(base_data, aux_data, radius):\n",
    "    \"\"\"Count the number of nearby records within a specified radius.\"\"\"\n",
    "\n",
    "    # Convert latitude and longitude to radians for haversine metric\n",
    "    base_data_rad = np.radians(base_data[['latitude', 'longitude']])\n",
    "    aux_data_rad = np.radians(aux_data[['latitude', 'longitude']])\n",
    "\n",
    "    # Initialize KNN\n",
    "    knn = NearestNeighbors(algorithm='ball_tree', metric='haversine')\n",
    "    knn.fit(aux_data_rad)\n",
    "\n",
    "    # Find records within the specified radius\n",
    "    indices = knn.radius_neighbors(base_data_rad, radius=radius/6371.0, return_distance=False)  # radius/6371.0 to convert km to radians\n",
    "\n",
    "    # Count the number of records within the radius for each main record\n",
    "    counts = [len(ind) for ind in indices]\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Define radii for each auxiliary dataset\n",
    "radii = {\n",
    "    \"school\": 5.0,  # 5 km\n",
    "    \"mrt_adjacent\": 0.3,  # 0.3 km\n",
    "    \"mrt_nearby\": 1.0,  # 1 km\n",
    "    \"mall\": 8.0  # 8 km\n",
    "}\n",
    "\n",
    "# Process datasets\n",
    "datasets = [train_data, test_data]\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    dataset[f'school_count'] = count_nearby_records(dataset, aux_datasets[\"school\"], radii[\"school\"])\n",
    "    dataset[f'mall_count'] = count_nearby_records(dataset, aux_datasets[\"mall\"], radii[\"mall\"])\n",
    "\n",
    "    # For MRT stations\n",
    "    for mrt_key in [\"mrt_plan\", \"mrt_exist\"]:\n",
    "        for radius_key in [\"mrt_adjacent\", \"mrt_nearby\"]:\n",
    "            col_name = f'{mrt_key}_{radius_key}_count'\n",
    "            dataset[col_name] = count_nearby_records(dataset, aux_datasets[mrt_key], radii[radius_key])\n",
    "\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Encoding Categorical Variables by Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 a) town, street_name, and subzone\n",
    "* group the dataframe for each categorical feature\n",
    "* compute each group's statistics and merge to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grouped_stats(df, group_column, target_column):\n",
    "    \"\"\"\n",
    "    Compute statistics for a given target column grouped by another column.\n",
    "    \"\"\"\n",
    "    grouped = df[[target_column, group_column]].groupby(group_column)\n",
    "\n",
    "    stats_functions = {\n",
    "        'size': 'size',\n",
    "        'mean': 'mean',\n",
    "        'std': 'std',\n",
    "        'median': 'median',\n",
    "        'max': 'max',\n",
    "        'min': 'min'\n",
    "    }\n",
    "\n",
    "    stats_list = []\n",
    "    for stat_name, func in stats_functions.items():\n",
    "        stat_df = grouped.agg(func).reset_index()\n",
    "        stat_df.columns = [group_column, f\"{group_column}_{stat_name}_{target_column}\"]\n",
    "        stats_list.append(stat_df)\n",
    "\n",
    "    stats_df = pd.concat(stats_list, axis=1)\n",
    "    stats_df = stats_df.loc[:,~stats_df.columns.duplicated()]\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "def get_stats(df, target_column, group_column):\n",
    "    \"\"\"\n",
    "    Compute grouped statistics for a target column based on a group column.\n",
    "\n",
    "    Parameters:\n",
    "    - target_column: Numeric columns to group with (e.g. price, bedrooms, bathrooms)\n",
    "    - group_column: Categorical columns to group on (e.g. manager_id, building_id)\n",
    "    \"\"\"\n",
    "    stats_df = compute_grouped_stats(df, group_column, target_column)\n",
    "    return df.merge(stats_df, on=group_column, how='left')\n",
    "\n",
    "datasets = [train_data, test_data]\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    for group_column in ['town', 'street_name', 'subzone']:\n",
    "        datasets[idx] = get_stats(dataset, 'floor_area_sqm', group_column)\n",
    "        # datasets[idx] = encode_drop(dataset, group_column)\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 b) town, street_name, and subzone\n",
    "* group the dataframe for each categorical feature\n",
    "* compute each group's statistics w.r.t the target and merge to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_stats_to_dataset(dataset, stats_df, group_column):\n",
    "    \"\"\"Merge the computed statistics to the dataset.\"\"\"\n",
    "    merged_dataset = dataset.merge(stats_df, on=group_column, how='left')\n",
    "    return merged_dataset\n",
    "\n",
    "datasets = [train_data, test_data]\n",
    "# Compute and merge statistics for each categorical column\n",
    "for group_column in ['town', 'street_name', 'subzone', 'flat_model', 'planning_area']:\n",
    "    stats_df = compute_grouped_stats(train_data, group_column, 'monthly_rent')\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        dataset = merge_stats_to_dataset(dataset, stats_df, group_column)\n",
    "        datasets[idx] = encode_drop(dataset, group_column)\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for idx, dataset in enumerate(datasets):\n",
    "    for group_column in ['latitude', 'longitude']:\n",
    "        datasets[idx] = encode_drop(dataset, group_column)\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Dimension Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.4 a) PCA"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "train_data_without_target = train_data.drop(columns=['monthly_rent'])\n",
    "\n",
    "# 1. Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data_standardized = scaler.fit_transform(train_data_without_target)\n",
    "\n",
    "# 2. Perform PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit(train_data_standardized)\n",
    "\n",
    "# 3. Calculate the explained variance ratio for each feature\n",
    "explained_variance_ratio = pca_result.explained_variance_ratio_\n",
    "# Calculate the cumulative sum of the explained variance ratio\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "# Find the number of components required to explain at least 90% of the variance\n",
    "num_components = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
    "\n",
    "# Plotting the explained variance ratio\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(explained_variance_ratio)), explained_variance_ratio, alpha=0.7, align='center')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.title('Contribution of Principal Components to Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display feature contributions\n",
    "feature_contributions = pd.DataFrame({\n",
    "    'Feature': train_data_without_target.columns,\n",
    "    'Contribution to Variance': explained_variance_ratio\n",
    "})\n",
    "\n",
    "print(feature_contributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "most_contribute_feature = np.array(feature_contributions['Feature'][:22])\n",
    "print(most_contribute_feature)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Methods - Single Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After EDA and data preprocessing, we can now apply data mining methods for prediction. In this section, we choose different algorithms to generate regression models, including Multiple Linear Regression, Random Forest, XGBoost and LightGBM. The general proposal is to split the preprocessed dataset into train/test data, train and validate with train data, and evaluate the optimized model performance with test data. Finally, compare the models to get a overall picture of how well these regression models perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different modelling methods may involve different data processing and feature engineering techniques. Therefore, instead of using the train_data/test_data directly, we create separate copies of the dataset for each method to generate the corresponding training/testing data for that particular method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mlr = train_data.copy()\n",
    "data_mlr.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature selection, we first base on Main Data primarily, and take several attributes 'school_count', 'mall_count', 'mrt_exist_mrt_nearby_count' in the Aux Data to take additional information into consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_mlr[['flat_type', 'flat_model_0', 'flat_model_1', 'flat_model_2', 'flat_model_3', 'flat_model_4', 'floor_area_sqm', 'latitude', 'longitude', 'planning_area_0', 'planning_area_1', 'planning_area_2',        'planning_area_3', 'planning_area_4', 'real_estate_age', 'rent_month',\n",
    "       'rent_year', 'school_count', 'mall_count', 'mrt_exist_mrt_nearby_count']]\n",
    "y = data_mlr['monthly_rent']\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization on numerical columns\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = ['floor_area_sqm', 'latitude', 'longitude', 'real_estate_age', 'school_count', 'mall_count', 'mrt_exist_mrt_nearby_count']\n",
    "X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n",
    "X_test[numerical_cols] = scaler.fit_transform(X_test[numerical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Linear Regression model, for simplicity here we first take all attributes into consideration\n",
    "mlr = LinearRegression()\n",
    "mlr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on training dataset\n",
    "\n",
    "We first use metrics to evaluate the model performance on the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = mlr.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "print('R^2:',metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:',metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing\n",
    "plt.scatter(y_train, y_pred_train)\n",
    "plt.xlabel(\"Actual Price\")\n",
    "plt.ylabel(\"Predicted Price\")\n",
    "plt.title(\"Actual vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions on MLR Models**\n",
    "\n",
    "There are some main assumptions on Multiple Regression Models, which are listed below:\n",
    "\n",
    "* Linearity\n",
    "* No Multi-Collinearity\n",
    "* Homoskedasticity\n",
    "* Independence of independent variable\n",
    "* Normality\n",
    "* Independence of errors\n",
    "\n",
    "We should conduct examinations on whether the assumptions above are satisfied in this context. Basic methods include using diagnostic plots for the relevant analysis. For example, we can use residual plots to check if homoskedasticity is violated or the errors' independency. Also, scatter plots of dependent versus independent variables can help us find non-linear relationships and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Linearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearity in MLR means that the relationship between the independent variables (features) and the dependent variable (target) is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_attributes = ['floor_area_sqm', 'latitude', 'longitude', 'real_estate_age', 'school_count', 'mall_count', 'mrt_exist_mrt_nearby_count']\n",
    "X_plot = X_train.loc[:,numerical_attributes]\n",
    "fig, axs = plt.subplots(ncols=7, nrows=1, figsize=(12, 4))\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i, k in enumerate(numerical_attributes):\n",
    "    sns.regplot(y=y_train, x=X_plot[k], ax=axs[i], scatter_kws={\"color\": \"teal\"}, line_kws={\"color\": \"red\"})\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from the plots that the relationship between the independent variables (features) and the dependent variable (target) is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **No Multi-Collinearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when two or more independent variables in the MLR model are highly correlated, making it difficult to separate their individual effects on the dependent variable. It can impact the reliability of your regression model's coefficients and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Correlation Matrix to have a look at the multi-collinearity across all variables first. Look for high correlation coefficients (typically greater than 0.7) between pairs of variables. High correlations suggest multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Correlation Matrix\n",
    "corrMatrix = X_train.corr()\n",
    "sns.heatmap(corrMatrix, annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flat_type` and `floor_area_sqm` has a correlation coefficient of 0.95, and `town` and `planning_area` has a correlation coefficient of 0.97. This may suggest multicollinearity.\n",
    "\n",
    "Note that `flat_type`, `town` and `planning_area` are  encoded and is not numeric values by nature, so we may consider other encoding methods for avoiding multi-collinearity across the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use another metrics Variance Inflation Factor (VIF) to measure how much the variance of the estimated regression coefficients is increased due to multi-collinearity. Calculate the VIF for each independent variable. High VIF values (typically greater than 10) indicate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_vif(X):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    return(vif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_vif(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, high VIF values (typically greater than 10) indicate multicollinearity. The form shows features \"flat_type\", \"floor_area_sqm\", \"real_estate_age\", \"rent_year\", \"school_count\" and \"mall_count\" may have issues of multicollinearity, among which the VIF value of \"rent_year\" is especially high, suggesting the potential inner problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Homoskedasticity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homoscedasticity means that the variance of the residuals (the differences between the observed values and the predicted values) is constant across all levels of the independent variables. If homoscedasticity is violated, it can lead to problems such as biased coefficient estimates and incorrect inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_pred_train, y_train - y_pred_train)\n",
    "plt.title(\"Residual vs Fit Plot\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there is a random scattering of points with no discernible pattern. As in a homoskedastic dataset, the points should be evenly scattered around the horizontal line at 0 (the residuals have constant variance), we can come to the conclusion that the Homoskedasticity is not violated.\n",
    "\n",
    "Also, we do not observe a funnel-shaped, fan-shaped pattern, or any other systematic change in the spread of residuals as the fitted values change, which suggests heteroskedasticity (the variance of residuals is not constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Independence of independent variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLR assumes that independent variables are not perfectly correlated with each other (i.e., they are independent). Violations of this assumption can lead to unstable coefficient estimates and difficulties in interpreting their individual effects. We perform check of the independence, and some typical ones are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of X variables-'floor_area_sqm'\n",
    "plt.scatter(X_train['floor_area_sqm'], y_train - y_pred_train)\n",
    "plt.title(\"Floor Area Size vs Residuals\")\n",
    "plt.xlabel(\"floor_area_sqm\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of X variables-'latitude'\n",
    "plt.scatter(X_train['latitude'], y_train - y_pred_train)\n",
    "plt.title(\"Latitude vs Residuals\")\n",
    "plt.xlabel(\"latitude\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of X variables-'longitude'\n",
    "plt.scatter(X_train['longitude'], y_train - y_pred_train)\n",
    "plt.title(\"Longitude vs Residuals\")\n",
    "plt.xlabel(\"latitude\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above all show that residuals do appear randomly and symmetrically distributed around zero under all conditions, which prove the independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Normality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLR assumes that the residuals (the differences between the observed and predicted values) are normally distributed. Deviations from normality can impact the validity of statistical inference, such as hypothesis tests and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of residuals\n",
    "sns.distplot(y_train - y_pred_train)\n",
    "plt.title(\"Normality of Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot above that the residuals are normally distributed, which means the normality assumption is satiefied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Independence of errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independence of errors states that the residuals (the differences between the observed values and the predicted values) should be independent of each other. Violations of this assumption can lead to incorrect parameter estimates, unreliable hypothesis tests, and inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use *Durbin-Watson Test* for checking independence of errors. This statistical test checks for the presence of autocorrelation in the residuals. A Durbin-Watson statistic value close to 2 indicates no autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "durbin_watson_statistic = durbin_watson(y_train - y_pred_train)\n",
    "print(f'Durbin-Watson Statistic: {durbin_watson_statistic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Durbin-Watson statistic value is close to 2, which means there is no autocorrelation (i.e. the assmuption is satisfied)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on test dataset\n",
    "\n",
    "After the evalution on the training dataset, as well as the checks for MLR assmuptions and corresponding model refinement, we can use the model as the final  MLR model and evaluate its performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = mlr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_dict = {'Model':'Multiple Linear Regression',\n",
    "          'R^2':metrics.r2_score(y_test, y_pred_test),\n",
    "          'Adjusted R^2':(1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)),\n",
    "          'MAE':metrics.mean_absolute_error(y_test, y_pred_test),\n",
    "          'MSE':metrics.mean_squared_error(y_test, y_pred_test),\n",
    "          'RMSE':np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))}\n",
    "\n",
    "mlr_metrics = pd.DataFrame.from_dict(mlr_dict, orient = 'index').T\n",
    "\n",
    "# Display model performance metrics\n",
    "mlr_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the training and testing result of the multiple linear regression model, we could find that the R^2 and RMSE is not very ideal. This might due to we use a very intuitive selection of features, and the simple structure of linear regression model could not handle the complexity of such multiple dimensions. Also we have noticed that there are inner problems of the multiple linear regression model such as the multi-collinearity, which suggests a single MLR model is not an potimal choice in the current context. But still we could base on this model to provide insights on other models, for example the selection of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rf = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_rf.drop(['monthly_rent'], axis = 1)\n",
    "y = data_rf['monthly_rent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use GridSearchCV as a technique to systematically search through different combinations of hyperparameters to find the best set of hyperparameters for RF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    # 'max_depth': [90, 100], # Maximum number of levels in each decision tree\n",
    "    'max_depth': [80, 90, 100],\n",
    "    'max_features': [2, 3], # Maximum number of features considered for splitting a node\n",
    "    # 'min_samples_leaf': [1, 3], # Minimum number of data points allowed in a leaf node\n",
    "    'min_samples_leaf': [1, 3, 4, 5],\n",
    "    # 'n_estimators': [300, 600] # Number of trees in the forest\n",
    "    'n_estimators': [100, 300, 600]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(random_state = 42)\n",
    "rf_grid = GridSearchCV(estimator = rf_reg, param_grid = rf_param_grid, cv=5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best RF estimator based on best parameters gemerated from above\n",
    "rf = rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on training dataset\n",
    "\n",
    "We first use metrics to evaluate the model performance on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use RF regressor to predict on training set\n",
    "y_pred_train = rf.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "print('R^2:',metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:',metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_dict = {'Model':'Random Forest Regressor',\n",
    "          'R^2':metrics.r2_score(y_test, y_pred_test),\n",
    "          'Adjusted R^2':(1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)),\n",
    "          'MAE':metrics.mean_absolute_error(y_test, y_pred_test),\n",
    "          'MSE':metrics.mean_squared_error(y_test, y_pred_test),\n",
    "          'RMSE':np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))}\n",
    "\n",
    "rf_metrics = pd.DataFrame.from_dict(rf_dict, orient = 'index').T\n",
    "\n",
    "# Display model performance metrics\n",
    "rf_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Importance of the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_features = pd.DataFrame([X_train.columns, rf.feature_importances_]).T\n",
    "rf_features = rf_features.rename(columns={0: 'Feature', 1: 'Importance Score'})\n",
    "rf_features.sort_values(by = 'Importance Score', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xgb = train_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data_xgb.drop(['monthly_rent'], axis = 1)\n",
    "y = data_xgb['monthly_rent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost import plot_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_param_grid = {\"learning_rate\": [0.05, 0.1, 0.2], # Step size shrinkage used in update to prevents overfitting.\n",
    "                  \"max_depth\"        : [6, 8, 9, 10], # Maximum depth of a tree.\n",
    "                  \"min_child_weight\" : [1, 3, 5, 7], # Minimum number of instances required in a child node\n",
    "                  \"gamma\"            : [0.0, 0.1, 0.2, 0.3], # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "                  \"colsample_bytree\" : [0.3, 0.4, 0.6, 0.8] # Number of features supplied to a tree\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_reg = xgb.XGBRegressor(seed = 42, objective ='reg:squarederror')\n",
    "xgb_grid = GridSearchCV(estimator = xgb_reg, param_grid = xgb_param_grid, cv=5, n_jobs=-1, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best XGBoost estimator based on best parameters gemerated from above\n",
    "xgb = xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on training dataset\n",
    "\n",
    "We first use metrics to evaluate the model performance on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use XGBoost regressor to predict on training set\n",
    "y_pred_train = xgb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation metrics\n",
    "print('R^2:',metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:',metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_dict = {'Model':'XGBoost Regressor',\n",
    "          'R^2':metrics.r2_score(y_test, y_pred_test),\n",
    "          'Adjusted R^2':(1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1)),\n",
    "          'MAE':metrics.mean_absolute_error(y_test, y_pred_test),\n",
    "          'MSE':metrics.mean_squared_error(y_test, y_pred_test),\n",
    "          'RMSE':np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))}\n",
    "\n",
    "xgb_metrics = pd.DataFrame.from_dict(xgb_dict, orient = 'index').T\n",
    "\n",
    "# Display model performance metrics\n",
    "xgb_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Importance of the features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_features = pd.DataFrame([X_train.columns, xgb.feature_importances_]).T\n",
    "xgb_features = xgb_features.rename(columns={0: 'Feature', 1: 'Importance Score'})\n",
    "xgb_features.sort_values(by = 'Importance Score', ascending = False).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_importance(xgb, max_num_features = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.concat([mlr_metrics])\n",
    "df_metrics.sort_values(by = 'RMSE', ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See from the metrics comparison above, XXXXXXX achieves the best performance on the test set with an RMSE of XXXXXXX."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Method - Ensemble Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************Linear model*******************\n",
    "def get_model():\n",
    "    net = nn.Sequential(nn.Linear(in_features, 64),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(64, 32),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(32, 1),)\n",
    "    return net\n",
    "\n",
    "def log_rmse(net, features, labels):\n",
    "    clipped_preds = torch.clamp(net(features), 1, float('inf'))\n",
    "    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))\n",
    "    return rmse.item()\n",
    "\n",
    "def train(net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, weight_decay, batch_size):\n",
    "    train_ls, test_ls = [], []\n",
    "    train_iter = d2l.load_array((train_features, train_labels), batch_size)\n",
    "    # Adam optimization\n",
    "    optimizer = torch.optim.Adam(net.parameters(),\n",
    "                                 lr = learning_rate,\n",
    "                                 weight_decay = weight_decay)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for X, y in train_iter:\n",
    "            optimizer.zero_grad()\n",
    "            l = loss(net(X), y)\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        train_ls.append(log_rmse(net, train_features, train_labels))\n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "    return train_ls, test_ls\n",
    "\n",
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = torch.cat([X_train, X_part], 0)\n",
    "            y_train = torch.cat([y_train, y_part], 0)\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,\n",
    "           batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_model()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],\n",
    "                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],\n",
    "                     legend=['train', 'valid'], yscale='log')\n",
    "        print(f'fold {i + 1}, train log rmse {float(train_ls[-1]):f}, '\n",
    "              f'valid log rmse {float(valid_ls[-1]):f}')\n",
    "    return train_l_sum / k, valid_l_sum / k\n",
    "\n",
    "def train_and_pred(train_features, test_features, train_labels, test_data,\n",
    "                   num_epochs, lr, weight_decay, batch_size):\n",
    "    net = get_model()\n",
    "    train_ls, _ = train(net, train_features, train_labels, None, None,\n",
    "                        num_epochs, lr, weight_decay, batch_size)\n",
    "    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',\n",
    "             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')\n",
    "    print(f'train log rmse {float(train_ls[-1]):f}')\n",
    "    preds = net(test_features).detach().numpy()\n",
    "    df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "    test_data['Predicted'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([df_id['Id'], test_data['Predicted']], axis=1)\n",
    "    return submission, train_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.reset_index(drop=True, inplace=True)\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len = train_data.shape[0] * 0.8\n",
    "numeric_attributes=['flat_type', 'floor_area_sqm', 'latitude', 'longitude']\n",
    "train_data_linear = train_data.loc[:train_len-1,numeric_attributes]\n",
    "train_data_rent = train_data.loc[:train_len-1,['monthly_rent']]\n",
    "train_data_rent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_linear = train_data.loc[train_len:,numeric_attributes]\n",
    "test_data_linear.head()\n",
    "print(train_data_linear.shape, test_data_linear.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_attributes = torch.tensor(train_data_linear.values, dtype=torch.float32)\n",
    "test_attributes = torch.tensor(test_data_linear.values, dtype=torch.float32)\n",
    "train_labels_linear = torch.tensor(train_data_rent.values.reshape(-1, 1), dtype=torch.float32)\n",
    "loss = nn.MSELoss()\n",
    "in_features = train_attributes.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train linear model\n",
    "k, num_epochs, lr, weight_decay, batch_size = 5, 600, 0.01, 0, 64 # adjust hyperparameter\n",
    "train_l, valid_l = k_fold(k, train_attributes, train_labels_linear, num_epochs, lr,\n",
    "                          weight_decay, batch_size)\n",
    "print(f'{k}-fold cross-validation: average train log rmse: {float(train_l):f}, '\n",
    "      f'average test log rmse: {float(valid_l):f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict linear model\n",
    "submission, train_ls = train_and_pred(train_attributes, test_attributes, train_labels_linear, test_data,\n",
    "               num_epochs, lr, weight_decay, batch_size)\n",
    "# score = self_evaluation(submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('result/submission_linear_rmsle'+str(float(train_ls[-1]))[:6]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define error metrics\n",
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "# k-fold Cross Validation\n",
    "# set verbose=3 for more details\n",
    "def rmsle_cross_val(X_train, y_train, n_folds, model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n",
    "    return rmse\n",
    "\n",
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1(2) Self-Defined Linear NN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network model\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# RMSE loss function\n",
    "def log_rmse(preds, labels):\n",
    "    clipped_preds = torch.clamp(preds, 1, float('inf'))\n",
    "    return torch.sqrt(nn.MSELoss()(torch.log(clipped_preds), torch.log(labels)))\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    for X, y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, loss_fn):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_loader:\n",
    "            outputs = model(X)\n",
    "            loss = loss_fn(outputs, y)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Load data\n",
    "train_data.reset_index(drop=True, inplace=True)\n",
    "train_len = int(train_data.shape[0] * 0.8)\n",
    "numeric_attributes = ['flat_type', 'floor_area_sqm', 'latitude', 'longitude']\n",
    "\n",
    "train_features = torch.tensor(train_data.iloc[:train_len][numeric_attributes].values, dtype=torch.float32)\n",
    "train_labels = torch.tensor(train_data.iloc[:train_len]['monthly_rent'].values.reshape(-1, 1), dtype=torch.float32)\n",
    "test_features = torch.tensor(train_data.iloc[train_len:][numeric_attributes].values, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(train_features, train_labels)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Initialize model, optimizer, and loss\n",
    "model = LinearModel(train_features.shape[1])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training and evaluation\n",
    "num_epochs = 600\n",
    "for epoch in range(num_epochs):\n",
    "    train_model(model, train_loader, optimizer, loss_fn)\n",
    "    train_loss = evaluate_model(model, train_loader, log_rmse)\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_features)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission = pd.DataFrame({\n",
    "    'Id': range(len(predictions)),\n",
    "    'Predicted': predictions.squeeze().numpy()\n",
    "})\n",
    "\n",
    "submission.to_csv('result/submission_linear_rmsle'+str(float(train_ls[-1]))[:6]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 Lasso Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************Lasso model*******************\n",
    "# lasso_alphas = [0.00001, 0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005]\n",
    "# model_lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1000,alphas =lasso_alphas, random_state=42, cv=kfolds))\n",
    "# model_lasso = make_pipeline(RobustScaler(), Lasso(max_iter=1000, alpha =0.00005, random_state=42))\n",
    "model_lasso = Lasso(max_iter=1000, alpha =0.00005, tol=0.1, random_state=42)\n",
    "score_lasso = rmsle_cross_val(train_data_linear, train_data_rent, 5, model_lasso)\n",
    "print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score_lasso.mean(), score_lasso.std()))\n",
    "model_lasso.fit(train_data_linear.values, train_data_rent.values)\n",
    "model_lasso_pred = model_lasso.predict(test_data_linear.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_lasso_pred), columns = ['Predicted'])\n",
    "model_lasso_pred, df_pred['Predicted']\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_lasso_rmse'+str(float(score_lasso.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************ENet model*******************\n",
    "# enet_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "# enet_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "model_ENet = make_pipeline(RobustScaler(), ElasticNet(max_iter=3000, alpha=0.0005, l1_ratio=0.8, tol=0.2, random_state=42))\n",
    "score_ENet = rmsle_cross_val(train_data_linear, train_data_rent, 5, model_ENet)\n",
    "print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score_ENet.mean(), score_ENet.std()))\n",
    "model_ENet.fit(train_data_linear.values, train_data_rent.values)\n",
    "model_ENet_pred = model_ENet.predict(test_data_linear.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_ENet_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_enet_rmse'+str(float(score_ENet.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************Ridge model*******************\n",
    "ridge_alphas = [1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\n",
    "model_ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kfolds))\n",
    "# model_ridge = make_pipeline(RobustScaler(), Ridge(max_iter=1000, alpha=0.1))\n",
    "score_ridge = rmsle_cross_val(train_data_linear, train_data_rent, 5, model_ridge)\n",
    "print(\"Ridge score: {:.4f} ({:.4f})\\n\".format(score_ridge.mean(), score_ridge.std()))\n",
    "model_ridge.fit(train_data_linear.values, train_data_rent.values)\n",
    "model_ridge_pred = model_ENet.predict(test_data_linear.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_ridge_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_ridge_rmse'+str(float(score_ridge.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************SVR model*******************\n",
    "# C = [0.1, 0.5, 1, 50, 100, 1000]\n",
    "# epsilon = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10]\n",
    "# C:5 - 0.1163 (0.0032), C:10 - 0.1140 (0.0034), epsilon: 0.05 - 0.1105 (0.0039)\n",
    "model_svr = make_pipeline(RobustScaler(), SVR(C=10, epsilon=0.05, gamma='scale'))\n",
    "score_svr = rmsle_cross_val(train_data_linear, np.log(train_data_rent), 5, model_svr)\n",
    "print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score_svr.mean(), score_svr.std()))\n",
    "\n",
    "model_svr.fit(train_data_linear.values, np.log(train_data_rent.values))\n",
    "model_svr_pred = model_svr.predict(test_data_linear.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(np.exp(model_svr_pred)), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_svr_rmsle'+str(float(score_svr.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init train data and test data for non-linear model\n",
    "all_data_ensemble = train_data.copy()\n",
    "# test_data_linear = train_data.loc[train_len*0.8:,numeric_attributes]\n",
    "train_data_ensemble = train_data.copy().drop(columns = ['monthly_rent'])\n",
    "# test_data_ensemble = test_data.copy().drop(columns = ['Predicted'])\n",
    "test_data_ensemble = test_data.copy()\n",
    "train_data_rent = train_data.loc[:train_len/0.8,['monthly_rent']]\n",
    "train_data_ensemble.shape, test_data_ensemble.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ensemble.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_rent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************Random Forest model*******************\n",
    "# use grid search to produce a baseline rf model\n",
    "model_rf = RandomForestRegressor(oob_score=True, random_state=3, n_jobs=-1)\n",
    "params ={\n",
    "    'n_estimators': [1300],# [800, 900, 1000, 1200, 1500],\n",
    "    'min_samples_leaf': [1],# [1, 2, 3, 5, 10, 25],\n",
    "    'max_features': [0.5],# [None, 0.5, 'sqrt', 'log2'],\n",
    "    'max_depth': [35],# [5, 6, 7, 8, 10, 15, 20],\n",
    "    'min_samples_split': [2]# [2, 3, 4]\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "for g in ParameterGrid(params):\n",
    "    model_rf.set_params(**g)\n",
    "    model_rf.fit(train_data_ensemble, train_data_rent)\n",
    "    if model_rf.oob_score_ > best_score:\n",
    "        best_score = model_rf.oob_score_\n",
    "        best_grid = g\n",
    "        print('oob:', best_score, best_grid)\n",
    "# oob: 0.9764868974915827 {'max_depth': 35, 'max_features': 0.5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 1300}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check feature importance\n",
    "def RF_importance(m, df):\n",
    "    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}).sort_values('imp', ascending=False)\n",
    "\n",
    "model_rf = RandomForestRegressor(n_jobs=-1, n_estimators=1300, oob_score=True, max_depth=35, min_samples_leaf=1, min_samples_split=2, max_features=0.5)\n",
    "model_rf.fit(train_data_ensemble, train_data_rent)\n",
    "feature_imp = RF_importance(model_rf, train_data_ensemble)\n",
    "print(model_rf.oob_score_)\n",
    "print(feature_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "Threshold = 0.0009\n",
    "to_keep = feature_imp[feature_imp.imp>Threshold].cols\n",
    "to_keep = [col for _, col in to_keep.items()]\n",
    "print(to_keep)\n",
    "df_keep = train_data_ensemble[to_keep].copy()\n",
    "model_rf = RandomForestRegressor(n_jobs=-1, n_estimators=1300, oob_score=True, max_depth=35, min_samples_leaf=1, min_samples_split=2, max_features=0.5)\n",
    "model_rf.fit(df_keep, train_data_rent)\n",
    "score_rf = rmsle_cross_val(train_data_ensemble, train_data_rent, 5, model_rf)\n",
    "print(model_rf.oob_score_, score_rf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep selection\n",
    "# keep_cols imp >= 0.01\n",
    "keep_cols = ['rent_approval_date', 'latitude', 'longitude', 'floor_area_sqm', 'flat_type', 'lease_commence_date', 'flat_model', 'planning_area', 'town', 'region']\n",
    "cols = to_keep\n",
    "scores = []\n",
    "feats = []\n",
    "for col in cols:\n",
    "    tmp = to_keep.copy()\n",
    "    if col in keep_cols:\n",
    "        continue\n",
    "    tmp.remove(col)\n",
    "    df_tmp = train_data_ensemble[tmp].copy()\n",
    "    model_rf = RandomForestRegressor(n_jobs=-1, n_estimators=1300, oob_score=True, max_depth=35, min_samples_leaf=1, min_samples_split=2, max_features=0.5)\n",
    "    model_rf.fit(df_tmp, train_data_rent)\n",
    "    scores.append(model_rf.oob_score_)\n",
    "    feats.append(col)\n",
    "\n",
    "to_del = sorted(zip(scores, feats), reverse=True)\n",
    "to_del"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuning\n",
    "# delete increase less than 0.0001\n",
    "to_keep_final = ['latitude', 'lease_commence_date', 'longitude', 'flat_type', 'flat_model', 'floor_area_sqm',\n",
    "                 'rent_approval_date']\n",
    "train_data_final = train_data_ensemble[to_keep_final].copy()\n",
    "model_rf = RandomForestRegressor(oob_score=True, n_jobs=-1)\n",
    "params ={\n",
    "    'n_estimators': [1200, 1250,1280],# [1200, 1250, 1280, 1300, 1350, 1400],\n",
    "    'min_samples_leaf': [1],# [1, 2, 3, 5, 10, 25],\n",
    "    'max_features': [0.5],# [0.5, 'sqrt', 'log2'],\n",
    "    'max_depth': [35],# [5, 6, 7, 8],\n",
    "    'min_samples_split': [2]# [2, 3, 4]\n",
    "}\n",
    "\n",
    "best_score = 0\n",
    "for g in ParameterGrid(params):\n",
    "    model_rf.set_params(**g)\n",
    "    model_rf.fit(train_data_final, train_data_rent)\n",
    "    if model_rf.oob_score_ > best_score:\n",
    "        score_rf = rmsle_cross_val(train_data_final, train_data_rent, 5, model_rf)\n",
    "        best_score = model_rf.oob_score_\n",
    "        best_grid = g\n",
    "        print('best oob:', best_score, best_grid, 'score:', score_rf.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rf = RandomForestRegressor(n_jobs=-1, n_estimators=1250, max_depth=35, min_samples_leaf=1, min_samples_split=2, max_features=0.5)\n",
    "model_rf.fit(train_data_final, train_data_rent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_final = test_data_ensemble[to_keep_final]\n",
    "# print(test_data_final)\n",
    "model_rf_pred=model_rf.predict(test_data_final)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_rf_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_rf_'+str(best_score)[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************GBoost model*******************\n",
    "model_GBoost = GradientBoostingRegressor(n_estimators=7000, learning_rate=0.1,\n",
    "                                   max_depth=5, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10,\n",
    "                                   loss='huber', random_state =42)\n",
    "score_GBoost = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_GBoost)\n",
    "print(\"GradientBoostingRegressor score: {:.4f} ({:.4f})\\n\".format(score_GBoost.mean(), score_GBoost.std()))\n",
    "\n",
    "model_GBoost.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_gb_pred = model_GBoost.predict(test_data_ensemble.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_gb_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_gb_rmsle'+str(float(score_GBoost.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************XGB model*******************\n",
    "# xgb_params = dict(\n",
    "#     max_depth=6,           # maximum depth of each tree - try 2 to 10\n",
    "#     learning_rate=0.01,    # effect of each tree - try 0.0001 to 0.1\n",
    "#     n_estimators=1000,     # number of trees (that is, boosting rounds) - try 1000 to 8000\n",
    "#     min_child_weight=1,    # minimum number of houses in a leaf - try 1 to 10\n",
    "#     colsample_bytree=0.7,  # fraction of features (columns) per tree - try 0.2 to 1.0\n",
    "#     subsample=0.7,         # fraction of instances (rows) per tree - try 0.2 to 1.0\n",
    "#     reg_alpha=0.5,         # L1 regularization (like LASSO) - try 0.0 to 10.0\n",
    "#     reg_lambda=1.0,        # L2 regularization (like Ridge) - try 0.0 to 10.0\n",
    "#     num_parallel_tree=1,   # set > 1 for boosted random forests\n",
    "# )\n",
    "model_xgb = xgb.XGBRegressor(learning_rate=0.01, gamma=0,\n",
    "                             max_depth=5, min_child_weight = 1,\n",
    "                             n_estimators=6000, colsample_bytree=0.6,\n",
    "                             reg_alpha=0.5, reg_lambda=1.0,\n",
    "                             subsample=0.7, random_state = 42, nthread = -1)\n",
    "score_xgb = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_xgb)\n",
    "print(\"XGB score: {:.4f} ({:.4f})\\n\".format(score_xgb.mean(), score_xgb.std()))\n",
    "\n",
    "model_xgb.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_xgb_pred = model_xgb.predict(test_data_ensemble.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_xgb_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_xgb_rmsle'+str(float(score_xgb.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************LightGBM model*******************\n",
    "# Use large max_bin (may be slower)\n",
    "# Use small learning_rate with large num_iterations\n",
    "# Use large num_leaves (may cause over-fitting)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',\n",
    "                       num_leaves=25, learning_rate=0.05,\n",
    "                       max_bin = 200, n_estimators=1000,\n",
    "                       random_state=42)\n",
    "score_lgb = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_lgb)\n",
    "print(\"LGB score: {:.4f} ({:.4f})\\n\".format(score_lgb.mean(), score_lgb.std()))\n",
    "\n",
    "model_lgb.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_lgb_pred = model_lgb.predict(test_data_ensemble.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_lgb_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_lgb_rmsle'+str(float(score_lgb.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************RF model*******************\n",
    "model_RF = RandomForestRegressor(n_jobs=-1, n_estimators=1250, oob_score=True, max_depth=35, min_samples_leaf=1, min_samples_split=2, max_features=0.5)\n",
    "model_RF.fit(train_data_ensemble.values, train_data_rent.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************Stacking model*******************\n",
    "model_stack = StackingCVRegressor(regressors=(model_GBoost, model_xgb, model_lgb),\n",
    "                                meta_regressor=model_rf,\n",
    "                                use_features_in_secondary=True)\n",
    "score_stack = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_stack)\n",
    "print(\"Stacked score: {:.4f} ({:.4f})\\n\".format(score_stack.mean(), score_stack.std()))\n",
    "# fitting stacked model\n",
    "model_stack.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_stack_pred = model_stack.predict(test_data_ensemble.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_stack_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('/submission_stacked_rmsle'+str(float(score_stack.mean()))[:7]+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *******************Blend models/Weighting*******************\n",
    "def blended_predictions(X):\n",
    "    return ((0.05 * model_GBoost.predict(X)) + \\\n",
    "            (0.1 * model_xgb.predict(X)) + \\\n",
    "            (0.1 * model_lgb.predict(X)) + \\\n",
    "            (0.55 * model_RF.predict(X)) + \\\n",
    "            (0.2 * model_stack.predict(X)))\n",
    "\n",
    "model_blend_pred = blended_predictions(train_data_ensemble.values)\n",
    "score_blend = rmse(np.log(train_data_rent.values), np.log(model_blend_pred))\n",
    "print(score_blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_blended_pred = blended_predictions(test_data_ensemble.values)\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_blended_pred), columns = ['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "# self_evaluation(submission)\n",
    "submission.to_csv('result/submission_blended_rmsle'+str(score_blend)[:7]+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
