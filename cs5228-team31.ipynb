{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "# Plotting and visualization\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from scipy.stats import probplot\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Set global font size for Seaborn\n",
    "sns.set(font_scale=1.4)  # This will scale the fonts by a factor of 1.4\n",
    "# Set the default font size to 18 for all text elements\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "# PyTorch and deep learning\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# Preprocessing and feature engineering\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from geopy.distance import geodesic\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import category_encoders as ce\n",
    "import pylab\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "\n",
    "# Modeling and evaluation\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, ElasticNetCV, LassoCV, Lasso, ElasticNet, Ridge\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, ParameterGrid, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "import lightgbm as lgb\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Miscellaneous\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from encoders import *\n",
    "from featureSelection import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main data\n",
    "train_data_path = './dataset/train.csv'\n",
    "test_data_path = './dataset/test.csv'\n",
    "\n",
    "# auxiliary data\n",
    "mrt_exist_data_path = './dataset/auxiliary-data/sg-mrt-existing-stations.csv'\n",
    "mrt_planned_data_path = './dataset/auxiliary-data/sg-mrt-planned-stations.csv'\n",
    "mall_data_path = './dataset/auxiliary-data/sg-shopping-malls.csv'\n",
    "primary_school_data_path = './dataset/auxiliary-data/sg-primary-schools.csv'\n",
    "coe_price_data_path = './dataset/auxiliary-data/sg-coe-prices.csv'\n",
    "stock_price_data_path = './dataset/auxiliary-data/sg-stock-prices.csv'\n",
    "\n",
    "# Load Data\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "test_data['monthly_rent'] = 0\n",
    "mrt_exist_data = pd.read_csv(mrt_exist_data_path)\n",
    "mrt_planned_data = pd.read_csv(mrt_planned_data_path)\n",
    "mall_data = pd.read_csv(mall_data_path)\n",
    "primary_school_data = pd.read_csv(primary_school_data_path)\n",
    "coe_price_data = pd.read_csv(coe_price_data_path)\n",
    "stock_price_data = pd.read_csv(stock_price_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 新加坡的住宅物业: 二房式灵活组屋（2-room flexi flat）、三房式组屋（3-room flat）、四房式组屋（4-room flat）、五房式组屋（5-room flat）、三代同堂组屋（3Gen flat）、公寓式组屋（Executive flat）"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flat_type\n",
    "train_data['flat_type'] = train_data['flat_type'].str.replace(\"-\", \" \")\n",
    "test_data['flat_type'] = test_data['flat_type'].str.replace(\"-\", \" \")\n",
    "\n",
    "# mrt_planned\n",
    "mrt_planned_data['opening_year'].replace('TBA', pd.NA, inplace=True)\n",
    "mrt_planned_data.dropna(subset=['opening_year'], inplace=True)\n",
    "# only include those planned within 3 years, due to the policy restriction\n",
    "mrt_planned_data = mrt_planned_data[(mrt_planned_data['opening_year']).astype(int) < 2029]\n",
    "\n",
    "train_data.head()\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. EDA - Main Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 General Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Visualization of General Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_data,height=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.4 b) Feature Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Compute the correlation matrix\n",
    "# corr_matrix = train_data.corr()\n",
    "#\n",
    "# # Plot the heatmap\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "# plt.title('Feature Correlation Heatmap')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "There is no null value for any column in the dataset.\n",
    "The scale of numerical data varies significantly, remember to normalize before analysis\n",
    "There are natural groups but not clear, further observation required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Univariate Analysis - Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.4 a) town & subarea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "town_counts = train_data['town'].value_counts()\n",
    "planning_area_counts = train_data['planning_area'].value_counts()\n",
    "\n",
    "# Merge the two Series into a single DataFrame\n",
    "combined_df = pd.DataFrame({'Town': town_counts, 'Planning Area': planning_area_counts}).fillna(0)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Width of a bar\n",
    "width = 0.4\n",
    "\n",
    "# Positions of bars on x axis\n",
    "r1 = range(len(combined_df))\n",
    "r2 = [x + width for x in r1]\n",
    "\n",
    "# Creating bars\n",
    "plt.bar(r1, combined_df['Town'], width=width, label='Town', color='blue', edgecolor='grey')\n",
    "plt.bar(r2, combined_df['Planning Area'], width=width, label='Planning Area', color='red', edgecolor='grey')\n",
    "\n",
    "# Title & Subtitle\n",
    "plt.title('Comparing Town and Planning Area Counts', fontweight='bold')\n",
    "\n",
    "# X axis\n",
    "plt.xlabel('Areas', fontweight='bold')\n",
    "plt.xticks([r + width for r in range(len(combined_df))], combined_df.index, rotation=90)\n",
    "\n",
    "# Y axis\n",
    "plt.ylabel('Counts', fontweight='bold')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 b) flat_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['flat_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 c) flat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_model_counts = train_data['flat_model'].value_counts()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15,6))  # Adjust the figure size\n",
    "sns.barplot(y=flat_model_counts.index, x=flat_model_counts.values, palette='viridis')  # Use y for town names to get a horizontal bar plot\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title('Number of Occurrences by Flat Model')\n",
    "plt.xlabel('Number of Occurrences')\n",
    "plt.ylabel('Flat Model')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data['flat_model'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 d) furnished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['furnished'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 e) latitude & longitude\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# latitude = train_data['latitude']\n",
    "# longitude = train_data['longitude']\n",
    "#\n",
    "# avg_lat, avg_lon = latitude.mean(), longitude.mean()\n",
    "#\n",
    "# # Create a base map\n",
    "# m = folium.Map(location=[avg_lat, avg_lon], zoom_start=12)\n",
    "#\n",
    "# # Add the heat map\n",
    "# heat_data = [[lat, lon] for lat, lon in zip(train_data['latitude'], train_data['longitude'])]\n",
    "# HeatMap(heat_data).add_to(m)\n",
    "#\n",
    "# # Display the map if NEEDED\n",
    "# # m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 f) subzone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['subzone'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 g) region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['region'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 h) block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['block'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Univariate Analysis - Numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 a) monthly_rent statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 5))\n",
    "\n",
    "# Histogram\n",
    "ax1 = sns.histplot(x=train_data['monthly_rent'], color='teal', ax=ax[0, 0])\n",
    "ax1.set_xlabel('Monthly Rent')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Boxplot\n",
    "ax2 = sns.boxplot(x=train_data['monthly_rent'], ax=ax[0, 1], color='teal')\n",
    "ax2.set_xlabel('Monthly Rent')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "# Violin Plot\n",
    "ax3 = sns.violinplot(x=train_data['monthly_rent'], ax=ax[1, 0], color='teal')\n",
    "ax3.set_xlabel('Monthly Rent')\n",
    "ax3.set_ylabel('Density')\n",
    "\n",
    "# Probability Plot\n",
    "ax4 = probplot(train_data['monthly_rent'], plot=ax[1, 1])\n",
    "ax[1, 1].set_xlabel('Theoretical Quantiles')\n",
    "ax[1, 1].set_ylabel('Ordered Values')\n",
    "# Remove the title\n",
    "ax[1, 1].set_title('')\n",
    "\n",
    "plt.tight_layout()  # Adjust the layout\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 b) region v.s. monthly_rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('notebook', font_scale = 1.3)\n",
    "plt.figure(figsize=(15, 4))\n",
    "ax = sns.barplot(x=train_data['region'],\n",
    "                 y=train_data['monthly_rent'],\n",
    "                 palette=sns.color_palette(\"viridis\", n_colors=len(train_data['region'].unique())),\n",
    "                 ci=None)\n",
    "plt.ylabel('Monthly Rent');\n",
    "\n",
    "for p in ax.patches:\n",
    "    ax.annotate(int(p.get_height()), (p.get_x() + 0.4, p.get_height() + 1), ha='center', va='bottom', color='Black')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 c) flat_type v.s. monthly_rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # Replace \"executive\" with \"6-room\"\n",
    "    dataset['flat_type'] = dataset['flat_type'].replace('executive', '5 room')\n",
    "    # Extract the numeric part and convert it to integer\n",
    "    dataset['room_num'] = dataset['flat_type'].str.extract('(\\d)').astype(int)\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "train_data, test_data = datasets\n",
    "\n",
    "# Group by room_num to get average rent and average rent per room\n",
    "grouped_data = train_data.groupby('room_num').agg({\n",
    "    'monthly_rent': 'mean'\n",
    "})\n",
    "grouped_data['rent_per_room'] = grouped_data['monthly_rent'] / grouped_data.index\n",
    "\n",
    "# Plotting\n",
    "fig, ax1 = plt.subplots(figsize=(10, 3))  # Adjust the figure size for better visualization\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('Room Number')\n",
    "ax1.set_ylabel('Monthly Rent Mean', color=color)\n",
    "ax1.plot(grouped_data.index, grouped_data['monthly_rent'], color=color, linestyle='-', marker='o')  # Added markers for clarity\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax1.set_ylim([1500, 3000])  # Set y-axis limit for ax1 as specified\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('Rent per Room Mean', color=color)\n",
    "ax2.plot(grouped_data.index, grouped_data['rent_per_room'], color=color, linestyle='--', marker='x')  # Different linestyle and markers for distinction\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "ax2.set_ylim([500, 1000])  # Set y-axis limit for ax2 as specified\n",
    "\n",
    "# Add a title and grid for better visualization\n",
    "ax1.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 d) flat_model v.s. monthly_rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_data['flat_model'].unique())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mean_rent = train_data.groupby('flat_model')['monthly_rent'].mean().reset_index()\n",
    "model_mean_rent = model_mean_rent.sort_values('monthly_rent')\n",
    "\n",
    "sns.set_context('notebook', font_scale=1.3)\n",
    "plt.figure(figsize=(13, 4))\n",
    "\n",
    "# Create the barplot\n",
    "ax = sns.barplot(x='flat_model',  # 'flat_model' column from the grouped DataFrame\n",
    "                 y='monthly_rent',  # Mean 'monthly_rent' from the grouped DataFrame\n",
    "                 data=model_mean_rent,  # The grouped DataFrame with mean rent\n",
    "                 palette='viridis',\n",
    "                 ci=None)\n",
    "\n",
    "# Remove x-axis labels to replace them with annotations inside the bars\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "ax.set_xlabel('Flat Model')\n",
    "plt.ylabel('Mean Monthly Rent')\n",
    "\n",
    "# Loop over each bar in the plot\n",
    "for p in ax.patches:\n",
    "    # Get the bar height and x-position\n",
    "    height = int(p.get_height())\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "\n",
    "    # Annotate the height of the bar inside the bar\n",
    "    ax.annotate(f'{height}', (x, height), ha='center', va='center', color='black', xytext=(0, 10), textcoords='offset points', fontweight='bold')\n",
    "\n",
    "    # Annotate the flat_model name inside the bar\n",
    "    flat_model_name = model_mean_rent.loc[p.get_x() + p.get_width() / 2, 'flat_model']  # Get the corresponding flat_model name\n",
    "    ax.annotate(flat_model_name, (x, 0), ha='center', va='bottom', color='white', rotation=90, xytext=(0, 10), textcoords='offset points',fontweight='bold', fontsize=12.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T01:28:41.683154Z",
     "start_time": "2023-11-08T01:28:41.388768Z"
    }
   },
   "source": [
    "#### 1.5 e) house age v.s. monthly_rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_data = train_data.sort_values('lease_commence_date')\n",
    "\n",
    "# Calculate real estate age without modifying the original train_data\n",
    "age_rent_df = train_data[['lease_commence_date', 'monthly_rent']].copy()\n",
    "age_rent_df['HDB_age'] = 2023 - age_rent_df['lease_commence_date']\n",
    "\n",
    "# Group by real estate age and calculate mean rent\n",
    "grouped_data = age_rent_df.groupby('HDB_age').agg({\n",
    "    'monthly_rent': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.plot(grouped_data['HDB_age'], grouped_data['monthly_rent'], marker='o', linestyle='-')\n",
    "\n",
    "# Highlight the point for lease_commence_date = 1966 in red\n",
    "specific_year = 2023 - 1966\n",
    "mean_rent_for_year = grouped_data[grouped_data['HDB_age'] == specific_year]['monthly_rent'].values[0]\n",
    "plt.scatter(specific_year, mean_rent_for_year, color='orange', zorder=5)  # zorder=5 to draw the point on top of the line\n",
    "\n",
    "plt.xlabel('Real Estate Age (Years)')\n",
    "plt.ylabel('Mean Rent')\n",
    "plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data['lease_commence_date'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 f) floor_area_sqm v.s. monthly_rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "sns.histplot(train_data['floor_area_sqm'], bins=30, kde=False, color='orange', ax=ax[0])  # Adjust position as desired\n",
    "sns.scatterplot(x=train_data['floor_area_sqm'], y=train_data['monthly_rent'], ax=ax[1], color='green', alpha=0.5)  # Adjust position as desired\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 g) monthly_rent per sqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 4))\n",
    "\n",
    "# Left-hand side plot\n",
    "sns.histplot(train_data['monthly_rent'] / train_data['floor_area_sqm'], bins=30, kde=False, color='orange', ax=ax[0])\n",
    "ax[0].set_title('Distribution of Rent per sqm')  # Adding a title\n",
    "ax[0].set_xlabel('Rent per sqm')  # Setting x-label\n",
    "ax[0].set_ylabel('Frequency')  # Setting y-label\n",
    "\n",
    "# Right-hand side plot\n",
    "sns.scatterplot(x=train_data['floor_area_sqm'], y=train_data['monthly_rent'] / train_data['floor_area_sqm'], ax=ax[1], color='green', alpha=0.5)\n",
    "ax[1].set_title('Rent per sqm vs. Floor Area')  # Adding a title\n",
    "ax[1].set_xlabel('Floor Area (sqm)')  # Setting x-label\n",
    "ax[1].set_ylabel('Rent per sqm')  # Setting y-label\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 h) monthly_rent heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a base map\n",
    "zoom_level=12\n",
    "m = folium.Map(location=[train_data['latitude'].mean(), train_data['longitude'].mean()], zoom_start=zoom_level,\n",
    "               zoomControl=False, max_zoom=zoom_level, min_zoom=zoom_level, width='100%', height='100%')\n",
    "\n",
    "# Add the heatmap\n",
    "heat_data = [[row['latitude'], row['longitude'], row['monthly_rent']] for index, row in train_data.iterrows()]\n",
    "HeatMap(heat_data).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 i) monthly_rent clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Create a base map\n",
    "# zoom_level=12\n",
    "# m = folium.Map(location=[train_data['latitude'].mean(), train_data['longitude'].mean()], zoom_start=zoom_level,\n",
    "#                zoomControl=False, max_zoom=zoom_level, min_zoom=zoom_level, width='100%', height='100%')\n",
    "#\n",
    "# # Add the heatmap\n",
    "# heat_data = [[row['latitude'], row['longitude'], row['monthly_rent']] for index, row in train_data.iterrows()]\n",
    "# HeatMap(heat_data).add_to(m)\n",
    "# m\n",
    "#\n",
    "# # Extract and standardize the longitude and latitude features\n",
    "# coords = pd.concat([train_data[['longitude', 'latitude']], test_data[['longitude', 'latitude']]])\n",
    "# coords_scaled = StandardScaler().fit_transform(coords)\n",
    "#\n",
    "# # K-Means clustering\n",
    "# kmeans = KMeans(n_clusters=12, random_state=0)\n",
    "# kmeans.fit(coords_scaled)\n",
    "# train_data['kmeans_labels'] = kmeans.predict(train_data[['longitude', 'latitude']])\n",
    "# test_data['kmeans_labels'] = kmeans.predict(test_data[['longitude', 'latitude']])\n",
    "#\n",
    "# # Plotting\n",
    "# fig, ax = plt.pl(1, 2, figsize=(12, 5))\n",
    "#\n",
    "# # K-Means plot\n",
    "# ax[1].scatter(train_data['longitude'], train_data['latitude'], c=train_data['kmeans_labels'], cmap='rainbow', s=50)\n",
    "# ax[1].set_title('K-Means Clustering')\n",
    "# ax[1].set_xlabel('Longitude')\n",
    "# ax[1].set_ylabel('Latitude')\n",
    "#\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.5 j) monthly_rent mean and std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'train_data' is your DataFrame and 'rent_approval_date' and 'monthly_rent' are columns in it.\n",
    "\n",
    "# Copy the original train data to avoid modifying it\n",
    "split_date = train_data.copy()\n",
    "\n",
    "# Split the 'rent_approval_date' into separate year and month columns\n",
    "split_date[['rent_year', 'rent_month']] = split_date['rent_approval_date'].str.split('-', expand=True).astype(int)\n",
    "\n",
    "# Group by 'rent_year' and 'rent_month', then calculate mean and standard deviation for 'monthly_rent'\n",
    "monthly_rent_stats = split_date.groupby(['rent_year', 'rent_month'])['monthly_rent'].agg(\n",
    "    monthly_rent_mean='mean',\n",
    "    monthly_rent_volatility='std'\n",
    ").reset_index()\n",
    "\n",
    "# Calculate the yearly average rent\n",
    "yearly_rent_avg = split_date.groupby('rent_year')['monthly_rent'].mean().rename('monthly_rent_yearly_avg').reset_index()\n",
    "\n",
    "# Merge the yearly average rent with the monthly stats\n",
    "monthly_rent_stats = pd.merge(monthly_rent_stats, yearly_rent_avg, on='rent_year')\n",
    "\n",
    "# Create a 'year_month' column for plotting purposes\n",
    "monthly_rent_stats['year_month'] = monthly_rent_stats['rent_year'].astype(str) + '-' + monthly_rent_stats['rent_month'].astype(str)\n",
    "\n",
    "# Create a plot\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Plot the mean of the monthly rent\n",
    "ax.set_xlabel('Year-Month', fontsize=18)\n",
    "ax.set_ylabel('Rent Price', fontsize=18)\n",
    "ax.plot(monthly_rent_stats['year_month'], monthly_rent_stats['monthly_rent_mean'], color='tab:blue', label='Monthly Mean')\n",
    "ax.plot(monthly_rent_stats['year_month'], monthly_rent_stats['monthly_rent_volatility'], color='tab:red', linestyle='--', label='Monthly Volatility')\n",
    "ax.plot(monthly_rent_stats['year_month'], monthly_rent_stats['monthly_rent_yearly_avg'], color='tab:green', linestyle='-.', label='Yearly Average')\n",
    "ax.tick_params(axis='y', labelsize=16)\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=16)\n",
    "ax.legend()\n",
    "\n",
    "# Title of the plot\n",
    "# plt.title('Monthly Rent Mean, Yearly Average, and Volatility Over Time', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "fig.tight_layout()  # To ensure the subplot fits into the figure area\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(train_data['monthly_rent'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 EDA - Aux Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 a) coe\n",
    "* To register a vehicle, you must first bid for a COE.Category refers to different vehicle types. Some background info: https://onemotoring.lta.gov.sg/content/onemotoring/home/buying/upfront-vehicle-costs/certificate-of-entitlement--coe-.html\n",
    "1. category: Different categories might represent different types of vehicles, such as motorcycles, passenger cars, commercial vehicles, etc.\n",
    "2. month: the month in which the COE bid took place.\n",
    "3. bidding: the bidding round or session within the specified month.\n",
    "4. price: the successful bid price for the COE in that category and bidding session. It represents the cost to obtain the COE.\n",
    "5. quota: the number of COEs available for bidding in that category and session. It's the supply side of the equation.\n",
    "6. bids: the number of bids received for the available COEs in that category and session. It's the demand side of the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine 'year' and 'month' into a 'date' column and convert to datetime\n",
    "coe_price_data['date'] = pd.to_datetime(coe_price_data['year'].astype(str) + ' ' + coe_price_data['month'], format='%Y %B')\n",
    "coe_price_data.drop(columns=['year','month','bidding'], inplace=True)\n",
    "coe_price_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the \"date\" column to datetime format if it's not already\n",
    "coe_price_data['date'] = pd.to_datetime(coe_price_data['date'])\n",
    "\n",
    "# Extract year and month from the 'date' column\n",
    "coe_price_data['year'] = coe_price_data['date'].dt.year\n",
    "coe_price_data['month'] = coe_price_data['date'].dt.month\n",
    "\n",
    "# Group by 'year', 'month', and 'category', then calculate the mean price\n",
    "monthly_coe_stats = coe_price_data.groupby(['year', 'month', 'category'])['price'].mean().reset_index()\n",
    "\n",
    "# Pivot the data to have categories as columns and fill NaNs with the previous value\n",
    "category_pivot = monthly_coe_stats.pivot_table(index=['year', 'month'], columns='category', values='price').fillna(method='ffill')\n",
    "\n",
    "# Group by 'rent_year' and 'rent_month', then calculate mean for 'monthly_rent'\n",
    "monthly_rent_stats = split_date.groupby(['rent_year', 'rent_month'])['monthly_rent'].mean().reset_index()\n",
    "\n",
    "# Combine the COE categories and rent price into one DataFrame for scaling\n",
    "combined_stats = category_pivot.reset_index()\n",
    "combined_stats['rent_price'] = monthly_rent_stats['monthly_rent']\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "combined_stats[['rent_price', 'a', 'b', 'c', 'e']] = scaler.fit_transform(combined_stats[['rent_price', 'a', 'b', 'c', 'e']])\n",
    "\n",
    "# Create a 'year_month' column for plotting purposes\n",
    "combined_stats['year_month'] = combined_stats['year'].astype(str) + '-' + combined_stats['month'].apply(lambda x: f'{x:02d}')\n",
    "\n",
    "# Sort the DataFrame based on 'year_month' to ensure the plot is in chronological order\n",
    "combined_stats.sort_values('year_month', inplace=True)\n",
    "\n",
    "# Set the figure size for better readability\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# First subplot for category-wise COE price trends\n",
    "plt.subplot(2, 1, 1)\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta']\n",
    "line_styles = ['-', '--', '-.', ':', (0, (3, 5, 1, 5))]\n",
    "for i, category in enumerate(['a', 'b', 'c', 'e']):\n",
    "    plt.plot(combined_stats['year_month'], combined_stats[category].rolling(window=3).mean(), label=f'Cat. {category.upper()}', color=colors[i], linestyle=line_styles[i], linewidth=2)\n",
    "plt.plot(combined_stats['year_month'], combined_stats['rent_price'].rolling(window=3).mean(), color='yellow', label='Standardized Monthly Rent', linewidth=2)\n",
    "plt.title('Standardized COE Price Trends by Category Year-Monthly', fontsize=18)\n",
    "# plt.xlabel('Year-Month', fontsize=18)\n",
    "plt.ylabel('Standardized Mean Value', fontsize=18)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Second subplot for mean COE price vs rent\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(combined_stats['year_month'], combined_stats[['a', 'b', 'c','e']].mean(axis=1).rolling(window=3).mean(), color='blue', label='Mean COE Price', linewidth=2)\n",
    "plt.plot(combined_stats['year_month'], combined_stats['rent_price'].rolling(window=3).mean(), color='red', label='Standardized Monthly Rent', linewidth=2)\n",
    "plt.title('Standardized Mean COE Price vs. Monthly Rent Year-Monthly', fontsize=18)\n",
    "plt.xlabel('Year-Month', fontsize=18)\n",
    "plt.ylabel('Standardized Mean Value', fontsize=18)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the figure size for better readability\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Define colors and line styles for the plot\n",
    "colors = ['blue', 'green', 'red', 'cyan', 'magenta']\n",
    "line_styles = ['--', '--', '--', '--', '--']  # Dashed lines for categories\n",
    "solid_line_style = '-'  # Solid line for mean COE and rent\n",
    "\n",
    "# Plot category-wise COE price trends\n",
    "for i, category in enumerate(['a', 'b', 'c', 'e']):\n",
    "    plt.plot(combined_stats['year_month'], combined_stats[category].rolling(window=3).mean(), label=f'Cat. {category.upper()}', color=colors[i], linestyle=line_styles[i], linewidth=2)\n",
    "\n",
    "# Plot the mean COE price (solid line)\n",
    "plt.plot(combined_stats['year_month'], combined_stats[['a', 'b', 'c', 'e']].mean(axis=1).rolling(window=3).mean(), color='black', label='Mean COE Price', linestyle=solid_line_style, linewidth=2)\n",
    "\n",
    "# Plot the standardized monthly rent (solid line)\n",
    "plt.plot(combined_stats['year_month'], combined_stats['rent_price'].rolling(window=3).mean(), color='yellow', label='Standardized Monthly Rent', linestyle=solid_line_style, linewidth=2)\n",
    "\n",
    "# Title and labels\n",
    "plt.title('Standardized COE Price Trends and Monthly Rent Year-Monthly', fontsize=18)\n",
    "plt.xlabel('Year-Month', fontsize=18)\n",
    "plt.ylabel('Standardized Mean Value', fontsize=18)\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1.6 b) primary_school\n",
    "1. singapore primary school choose: https://investingsg.com/keyinfo/schools-properties/. Key points:\n",
    "* 什么样的房子算学区房？\n",
    "    * 教育部说了 ”每一所小学都是好学校“。但是，民间排名一直都在。一般意义上是在名校一公里范围内，抽签概率好的学校可以扩大到两公里范围。如果做社区义工还要注意住址和小学是否在同一个GRC。\n",
    "* 租房租多久，什么时候开始租？\n",
    "    * 报名是按IC地址，所以没有明确开始租房日期，只要报名时IC地址改了就可以。报名成功后需住满30个月，这项规定同样适用于买的房子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary_school_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 c) mall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mall_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 d) mrt_planned\n",
    "* https://www.propertyguru.com.sg/property-guides/mrt-effect-on-property-prices-39498\n",
    "   * 0.3 KM: MRT-house\n",
    "    * 1KM: MRT-near-house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrt_planned_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6 e) mrt_exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrt_exist_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.7 f) stock_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_price_data['name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remove illiquid stocks\n",
    "mask = stock_price_data['name'].transform(lambda x: stock_price_data['name'].value_counts()[x] >= 100)\n",
    "liquid_stock_price_data = stock_price_data[mask]\n",
    "\n",
    "# compute monthly statistics\n",
    "monthly_stats = get_economic_indicator_feature(liquid_stock_price_data, 'stock', 'adjusted_close')\n",
    "\n",
    "# Create a new column 'year_month' for plotting purposes\n",
    "monthly_stats['year_month'] = monthly_stats['year'].astype(str) + '-' + monthly_stats['month'].astype(str)\n",
    "\n",
    "# Explicitly create a figure and set the size\n",
    "fig, ax1 = plt.subplots(figsize=(12, 4))\n",
    "\n",
    "# Plot the mean of the index on the primary y-axis\n",
    "ax1.set_xlabel('Year-Month', fontsize=14)\n",
    "ax1.set_ylabel('Index Mean', color='tab:blue', fontsize=18)\n",
    "ax1.set_ylim(0,60)\n",
    "line1 = ax1.plot(monthly_stats['year_month'], monthly_stats['index_price'], color='tab:blue', label='Index Mean')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\n",
    "ax1.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "\n",
    "# Instantiate a second y-axis sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim(0,60)\n",
    "ax2.set_ylabel('Index Volatility', color='tab:red', fontsize=18)\n",
    "line2 = ax2.plot(monthly_stats['year_month'], monthly_stats['index_volatility'], color='tab:red', label='Index Volatility')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\n",
    "\n",
    "# Title of the plot\n",
    "# plt.title('Index Mean and Volatility Over Time')\n",
    "\n",
    "# Combine the legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc=0, fontsize=18)\n",
    "\n",
    "# Adjust the layout and show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# Plot the mean of the index on the primary y-axis\n",
    "ax1.set_xlabel('Year-Month', fontsize=14)\n",
    "ax1.set_ylabel('Index Mean', color='tab:blue', fontsize=18)\n",
    "ax1.set_ylim(0,60)\n",
    "line1 = ax1.plot(monthly_stats['year_month'], monthly_stats['index_price'], color='tab:blue', label='Index Mean')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue', labelsize=14)\n",
    "ax1.tick_params(axis='x', rotation=45, labelsize=14)\n",
    "\n",
    "# Instantiate a second y-axis sharing the same x-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim(0,60)\n",
    "ax2.set_ylabel('Index Volatility', color='tab:red', fontsize=18)\n",
    "line2 = ax2.plot(monthly_stats['year_month'], monthly_stats['index_volatility'], color='tab:red', label='Index Volatility')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:red', labelsize=14)\n",
    "\n",
    "# Title of the plot\n",
    "# plt.title('Index Mean and Volatility Over Time')\n",
    "\n",
    "# Combine the legends\n",
    "lines = line1 + line2\n",
    "labels = [l.get_label() for l in lines]\n",
    "ax1.legend(lines, labels, loc=0, fontsize=18)\n",
    "\n",
    "# Adjust the layout and show the plot\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 Observation - Main Data\n",
    "1. monthly rent\n",
    "* it's not heavily skewed, but there are definately outliers\n",
    "* regions - the average rent across different regions are roughly the same\n",
    "* flat_type - the average rent is positively correlated to the #room\n",
    "* flat_model - those with extremely low frequency in flat_model usually have a high price\n",
    "* lease_commence_date - graph indicates the rent is increasing yearly, and exhibit up and downs in each decade. this can be combined with the auxiliary data to analyze the influence of economics\n",
    "* latitude&longitude - observe from a higher angle, there are definately groups\n",
    "\n",
    "2. region\n",
    "* the rent data across regions varies less than 1%, this feature is providing very limited infomation\n",
    "\n",
    "3. town & planning_area\n",
    "* these two features' value overlaps, and there are 3 extra values in planning_area. binary encoded with other geographical features?\n",
    "4. flat_type\n",
    "* 4/3/5 rooms are most popular, does that mean sufficient demand or supply? (may influence price)\n",
    "5. furnished\n",
    "* 100% furnished, can we drop this feature?\n",
    "6. latitude & longitude\n",
    "* there are actually naturally formed groups, how to group them properly?\n",
    "7. subzone\n",
    "* maybe can be used with the latitude * longitude?\n",
    "8. block\n",
    "* What is block_NO: https://www.quora.com/Why-did-Singapore-start-having-HDB-block-numbers-with-letters-e-g-172A-B-C\n",
    "* To sum up: each number is a location, and the following letter is the age. However, we have location & lease date info, thus drop this\n",
    "9. Rent/SQM\n",
    "* the distribution shows a significant pattern compared to seperated; thus we should include this feature in FE\n",
    "10. Grouping\n",
    "* According to the heatmap, there are 12 natural groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## 1.9 Observation - Aux Data\n",
    "1. coe_price_data\n",
    "* figure out whether the COE is a lagging or not. More specifically, different types of COE may vary. Main focus should be on Category_C: Goods vehicle&bus, and Category_D: Motorcycle v.s. Category_A and Category_B. The former are used as productivity materials, while the latter is consumption.\n",
    "2. primary_school_data\n",
    "* for each school, find its KNN house, and statistics w.r.t rent: 1km_school_rent_mean, 1km_school_rent_median, 2km_school_rent_mode, ...;\n",
    "* for each record, find all primary_schools within 1km, 2km range, and average out the statistics before-mentioned.\n",
    "3. mall\n",
    "* for each mall, find its KNN house, and the statistics: mall_average_rent, mall_median_rent, mall_75_rent, mall_25_rend;\n",
    "* for each record, find the malls within 3km range, and average out the statistics before-mentioned.\n",
    "4. mrt_planned\n",
    "* for each record, find the #mrt_planned within 0.3KM and 1KM, with those beyond 3 years ones ignored.\n",
    "5. mrt_exist\n",
    "* for each record, find the #mrt_planned within 0.3KM and 1KM.\n",
    "6. stock_price\n",
    "* stock_price is a leading indicator of economics, while rent_price is lagging indicator. This relationship implies that stock_price can be used to predict the overall rent price.\n",
    "* stock_price includes 59 stocks, thus we can form an index, and compute the index value on each trading day. However, those extremely illiquid stocks should be removed.\n",
    "* Time-Series-Regression can have some weight in the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing\n",
    "## 2.1 Eliminate Redundant Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# block: drop this feature, refer to EDA observations\n",
    "# furnished: the value of this attribute for all data is \"yes\"\n",
    "# elevation: Similar to attribute \"furnished\"\n",
    "# region: drop this feature, refer to EDA observations\n",
    "dropped_features = ['furnished', 'elevation', 'region', 'block']\n",
    "\n",
    "datasets = [train_data, test_data]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    dataset.drop(columns=dropped_features, inplace=True)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.2 Remove Outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2 a) lease_commence_date Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = train_data[train_data['lease_commence_date'] >1966]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.2 b) monthly_rent Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the IQR\n",
    "Q1 = train_data['monthly_rent'].quantile(0.25)\n",
    "Q3 = train_data['monthly_rent'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define bounds for the outliers\n",
    "lower_bound = Q1 - 1.45 * IQR\n",
    "upper_bound = Q3 + 2.5 * IQR\n",
    "\n",
    "# Identify outliers\n",
    "outliers = (train_data['monthly_rent'] < lower_bound) | (train_data['monthly_rent'] > upper_bound)\n",
    "# Filter out the outliers\n",
    "train_data = train_data[~outliers]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data['rent_per_sqm'] = train_data['monthly_rent'] / train_data['floor_area_sqm']\n",
    "# Create a DataFrame for the regression\n",
    "data_for_model = pd.DataFrame({\n",
    "    'floor_area_sqm': train_data['floor_area_sqm'],\n",
    "    'rent_per_sqm': train_data['rent_per_sqm']\n",
    "})\n",
    "\n",
    "# Fit the quantile regression model at the 95th percentile\n",
    "quantile_model = smf.quantreg('rent_per_sqm ~ floor_area_sqm', data_for_model).fit(q=0.997)\n",
    "\n",
    "# Predict the boundary values\n",
    "train_data['boundary'] = quantile_model.predict(train_data[['floor_area_sqm']])\n",
    "\n",
    "# Identify outliers\n",
    "outliers = train_data['rent_per_sqm'] > train_data['boundary']\n",
    "\n",
    "# Remove outliers from the original train_data DataFrame\n",
    "train_data = train_data[~outliers].reset_index(drop=True)\n",
    "\n",
    "train_data.drop(columns=['boundary','rent_per_sqm'], inplace=True)\n",
    "train_data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import statsmodels.formula.api as smf\n",
    "# train_data_path = './dataset/train.csv'\n",
    "# train_data = pd.read_csv(train_data_path)\n",
    "#\n",
    "# suns_color_primary_light = '#5D3FD3'  # Lighter Purple\n",
    "# suns_color_secondary_light = '#FF8F66'  # Lighter Orange\n",
    "# suns_color_accent_light = '#FFC266'  # Lighter Yellow\n",
    "#\n",
    "# # First plot data and quantile regression\n",
    "# data = pd.DataFrame({\n",
    "#     'floor_area_sqm': train_data['floor_area_sqm'],\n",
    "#     'rent_per_sqm': train_data['monthly_rent'] / train_data['floor_area_sqm']\n",
    "# })\n",
    "#\n",
    "# # Fit the quantile regression model at the 99.95th percentile\n",
    "# quantile_model = smf.quantreg('rent_per_sqm ~ floor_area_sqm', data).fit(q=0.997)\n",
    "#\n",
    "# # Predict the values along the boundary\n",
    "# data['prediction'] = quantile_model.predict(data[['floor_area_sqm']])\n",
    "#\n",
    "# # Identify outliers\n",
    "# outliers_quantile = data['rent_per_sqm'] > data['prediction']\n",
    "#\n",
    "# # Second plot data and IQR\n",
    "# Q1 = train_data['monthly_rent'].quantile(0.25)\n",
    "# Q3 = train_data['monthly_rent'].quantile(0.75)\n",
    "# IQR = Q3 - Q1\n",
    "# lower_bound = Q1 - 1.45 * IQR\n",
    "# upper_bound = Q3 + 2.5 * IQR\n",
    "# outliers_iqr = (train_data['monthly_rent'] < lower_bound) | (train_data['monthly_rent'] > upper_bound)\n",
    "#\n",
    "# # Create a figure and a set of subplots\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "#\n",
    "# # Plot 'floor_area_sqm' vs. 'rent_per_sqm' with outliers\n",
    "# axs[1].scatter(data['floor_area_sqm'], data['rent_per_sqm'], alpha=0.5, color=suns_color_primary_light, label='Data')\n",
    "# axs[1].plot(data['floor_area_sqm'], data['prediction'], color=suns_color_secondary_light, label='99.7th Quantile Regression Line')\n",
    "# axs[1].scatter(data.loc[outliers_quantile, 'floor_area_sqm'], data.loc[outliers_quantile, 'rent_per_sqm'], alpha=0.9, color=suns_color_accent_light, label='Outliers')\n",
    "# axs[1].set_xlabel('Floor Area (sqm)')\n",
    "# axs[1].set_ylabel('Rent per Sqm')\n",
    "# axs[1].set_title('Floor Area vs Rent per Sqm with Outliers')\n",
    "# axs[1].legend()\n",
    "#\n",
    "# # Plot 'monthly_rent' with outliers\n",
    "# axs[0].scatter(train_data.index, train_data['monthly_rent'], alpha=0.5, color=suns_color_primary_light, label='Data')\n",
    "# axs[0].scatter(train_data.index[outliers_iqr], train_data['monthly_rent'][outliers_iqr], alpha=0.9, color=suns_color_accent_light, label='Outliers beyond [-1.45~2.5]*IQR')\n",
    "# axs[0].set_xlabel('Index')\n",
    "# axs[0].set_ylabel('Monthly Rent')\n",
    "# axs[0].set_title('Monthly Rent Distribution with Outliers')\n",
    "# axs[0].legend()\n",
    "#\n",
    "# # Adjust layout to prevent overlap\n",
    "# plt.tight_layout()\n",
    "#\n",
    "# # Show the plot\n",
    "# plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.3 Reformat Feature Values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cat_features = ['town', 'street_name', 'subzone', 'flat_model', 'planning_area']\n",
    "datasets = [train_data, test_data]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    for feature in cat_features:\n",
    "        dataset[feature] = dataset[feature].str.lower()\n",
    "\n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. Feature Engineering\n",
    "1. Binning of numerical variables - https://www.kaggle.com/competitions/two-sigma-connect-rental-listing-inquiries/discussion/32116\n",
    "2. create new features by encoding categorical variables using statistics (like mean, median, standard deviation, etc.) of continuous variables. https://www.kaggle.com/competitions/two-sigma-connect-rental-listing-inquiries/discussion/32123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Synergistically Combined Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1 a) Date Related Features\n",
    "1. lease_commence_date -> HDB_age\n",
    "2. rent_approval_date -> rent_year, rent_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # lease_commence_date -> HDB_age\n",
    "    dataset['HDB_age'] = 2023 - dataset['lease_commence_date']\n",
    "\n",
    "    # rent_approval_date -> rent_year, rent_month\n",
    "    split_date = dataset['rent_approval_date'].str.split('-')\n",
    "    dataset['rent_year'] = split_date.str[0].astype('int32')\n",
    "    dataset['rent_month'] = split_date.str[1].astype('int32')\n",
    "\n",
    "    dataset.drop(columns=['rent_approval_date'], inplace=True)\n",
    "    dataset.drop(columns=['lease_commence_date'], inplace=True)\n",
    "\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1 b) Encode the Urban Planning Areas\n",
    "* Planning Area (e.g., Bedok, Tampines, Jurong East)\n",
    "    * Subzone (e.g., Bedok North, Tampines East, Jurong Gateway)\n",
    "        * Town (e.g., Tampines Town, which may overlap with the Tampines Planning Area)\n",
    "                * Street (e.g., Tampines Street 11, which is a road within Tampines Town)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode the urban planning\n",
    "urban_planning = ['planning_area', 'subzone', 'town']\n",
    "train_data, test_data = train_data, test_data = encode_hierarchical(train_data, test_data, urban_planning)\n",
    "\n",
    "print(train_data[['planning_area', 'subzone', 'town', 'city_encoder']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.1 c) Room Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # Replace \"executive\" with \"6-room\"\n",
    "    dataset['flat_type'] = dataset['flat_type'].replace('executive', '6 room')\n",
    "    # Extract the numeric part and convert it to integer\n",
    "    dataset['room_num'] = dataset['flat_type'].str.extract('(\\d)').astype(int)\n",
    "    # Update the original datasets\n",
    "    dataset.drop(columns=['flat_type'], inplace=True)\n",
    "    datasets[idx] = dataset\n",
    "train_data, test_data = datasets\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Neighboring Facilities Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 a) \"Eigenvector Centrality\" Statistics\n",
    "* for each record of each aux-data: find its KNN houses and calculate statistics\n",
    "* for each house, find its KNN aux-record, and weighted-average their statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def weighted_avg(values, weights):\n",
    "    \"\"\"Compute the weighted average of values.\"\"\"\n",
    "    return np.average(values, weights=weights)\n",
    "\n",
    "def calculate_distance_to_mrt(aux_data, mrt_data, k=3):\n",
    "    \"\"\"Calculate the mean distance of each auxiliary data point to the k nearest MRTs.\"\"\"\n",
    "    aux_data_rad = np.radians(aux_data[['latitude', 'longitude']])\n",
    "    mrt_data_rad = np.radians(mrt_data[['latitude', 'longitude']])\n",
    "\n",
    "    knn = NearestNeighbors(n_neighbors=k, algorithm='ball_tree', metric='haversine')\n",
    "    knn.fit(mrt_data_rad)\n",
    "\n",
    "    distances, _ = knn.kneighbors(aux_data_rad)\n",
    "    mean_distances = distances.mean(axis=1)  # Calculate the mean distance to the k nearest MRTs\n",
    "\n",
    "    return mean_distances\n",
    "\n",
    "def get_knn_values(base_data, aux_data, k, mrt_data):\n",
    "    # Convert latitude and longitude to radians for haversine metric\n",
    "    base_data_rad = np.radians(base_data[['latitude', 'longitude']])\n",
    "\n",
    "    # Calculate mean distance to the k nearest MRTs for auxiliary data\n",
    "    aux_data['value'] = calculate_distance_to_mrt(aux_data, mrt_data, k)\n",
    "\n",
    "    # Initialize KNN for base data\n",
    "    knn_base = NearestNeighbors(n_neighbors=k, algorithm='ball_tree', metric='haversine')\n",
    "    knn_base.fit(base_data_rad)\n",
    "\n",
    "    # Compute the weighted mean 'value' of KNN schools and malls for each block\n",
    "    weighted_values = []  # This will store the weighted mean values for each row in base_data\n",
    "    for index, row in base_data.iterrows():\n",
    "        # Find KNN schools/malls for the block\n",
    "        distances, indices = knn_base.kneighbors([np.radians([row['latitude'], row['longitude']])])\n",
    "        distances = distances.flatten() + 1e-10  # Avoid division by zero\n",
    "        indices = indices.flatten()\n",
    "\n",
    "        # Ensure indices are within the bounds of aux_data\n",
    "        valid_indices = indices[indices < len(aux_data)]\n",
    "        if len(valid_indices) > 0:\n",
    "            knn_values = aux_data.iloc[valid_indices]['value']\n",
    "            weighted_mean_value = weighted_avg(knn_values, 1 / distances[:len(valid_indices)])\n",
    "        else:\n",
    "            weighted_mean_value = 0\n",
    "        weighted_values.append(weighted_mean_value)\n",
    "\n",
    "    return pd.Series(weighted_values)  # Return a Series of weighted mean values\n",
    "\n",
    "datasets = [train_data, test_data]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    dataset['school_value'] = get_knn_values(dataset, primary_school_data, k=5, mrt_data=mrt_exist_data)\n",
    "    dataset['mall_value'] = get_knn_values(dataset, mall_data, k=5, mrt_data=mrt_exist_data)\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 b) Lifecycle Facility Counts\n",
    "* compute the trans-cycles according to the gov plan\n",
    "    * MRT: 0.3 KM  MRT-house, 1KM: MRT-near-house\n",
    "    * Primary School: not explicit data, assume the 15min driving cycle of a school bus, 5KM\n",
    "    * Mall: 15min driving cycle, approximately 8KM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def count_nearby_records(base_data, aux_data, radius):\n",
    "    \"\"\"Count the number of nearby records within a specified radius.\"\"\"\n",
    "\n",
    "    # Convert latitude and longitude to radians for haversine metric\n",
    "    base_data_rad = np.radians(base_data[['latitude', 'longitude']])\n",
    "    aux_data_rad = np.radians(aux_data[['latitude', 'longitude']])\n",
    "\n",
    "    # Initialize KNN\n",
    "    knn = NearestNeighbors(algorithm='ball_tree', metric='haversine')\n",
    "    knn.fit(aux_data_rad)\n",
    "\n",
    "    # Find records within the specified radius\n",
    "    indices = knn.radius_neighbors(base_data_rad, radius=radius/6371.0, return_distance=False)  # radius/6371.0 to convert km to radians\n",
    "\n",
    "    # Count the number of records within the radius for each main record\n",
    "    counts = [len(ind) for ind in indices]\n",
    "\n",
    "    return counts\n",
    "\n",
    "# Define radii for each auxiliary dataset\n",
    "radii = {\n",
    "    \"school\": 5.0,  # 5 km\n",
    "    \"mrt_adjacent\": 0.3,  # 0.3 km\n",
    "    \"mrt_nearby\": 1.0,  # 1 km\n",
    "    \"mall\": 8.0  # 8 km\n",
    "}\n",
    "\n",
    "# Process datasets\n",
    "datasets = [train_data, test_data]\n",
    "aux_datasets = {\"school\": primary_school_data,\n",
    "                \"mrt_plan\": mrt_exist_data,\n",
    "                \"mrt_exist\": mrt_exist_data,\n",
    "                \"mall\": mall_data}\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    dataset[f'school_count'] = count_nearby_records(dataset, aux_datasets[\"school\"], radii[\"school\"])\n",
    "    dataset[f'mall_count'] = count_nearby_records(dataset, aux_datasets[\"mall\"], radii[\"mall\"])\n",
    "\n",
    "    # For MRT stations\n",
    "    for mrt_key in [\"mrt_plan\", \"mrt_exist\"]:\n",
    "        for radius_key in [\"mrt_adjacent\", \"mrt_nearby\"]:\n",
    "            col_name = f'{mrt_key}_{radii[radius_key]}_count'\n",
    "            dataset[col_name] = count_nearby_records(dataset, aux_datasets[mrt_key], radii[radius_key])\n",
    "\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.2 c) Min Distance to Facilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def haversine_distance_rad(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the haversine distance between two sets of latitude and longitude coordinates in radians.\n",
    "    \"\"\"\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    r = 6371  # Radius of Earth in kilometers\n",
    "    return c * r\n",
    "\n",
    "datasets = [train_data, test_data]\n",
    "aux_datasets = {\"school\": primary_school_data,\n",
    "                \"mrt_plan\": mrt_exist_data,\n",
    "                \"mrt_exist\": mrt_exist_data,\n",
    "                \"mall\": mall_data}\n",
    "\n",
    "# For each auxiliary dataset, compute the minimum distance to each record in train_data and test_data\n",
    "for aux_name, aux_data in aux_datasets.items():\n",
    "    # Convert latitude and longitude to radians for haversine metric\n",
    "    aux_data_rad = np.radians(aux_data[['latitude', 'longitude']])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        distances = []\n",
    "        base_data_rad = np.radians(dataset[['latitude', 'longitude']])\n",
    "\n",
    "        for index, row in base_data_rad.iterrows():\n",
    "            # Calculate distance from current record to all records in the auxiliary dataset\n",
    "            dist = haversine_distance_rad(row['latitude'], row['longitude'], aux_data_rad['latitude'].values, aux_data_rad['longitude'].values)\n",
    "            # Append the minimum distance to the distances list\n",
    "            distances.append(dist.min())\n",
    "\n",
    "        # Add the minimum distances as a new feature to the dataset\n",
    "        dataset[f'min_distance_to_{aux_name}'] = distances\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3 Economic Indicator Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.3 a) Stock index performance\n",
    "1. remove stocks with limited transcations\n",
    "2. compute the monthly index price and volatility based on adjusted_close_price\n",
    "3. concat to train/test where rent_month = month, rent_year = year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # Merge train_data with monthly_stats\n",
    "    dataset = dataset.merge(\n",
    "        monthly_stats,\n",
    "        left_on=['rent_year', 'rent_month'],\n",
    "        right_on=['year', 'month'],\n",
    "        how='left'\n",
    "    )\n",
    "    # Drop the 'year' and 'month' columns from merged_data as they are redundant\n",
    "    dataset.drop(columns=['year', 'month','year_month'], inplace=True)\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.3 b) COE trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, calculate the mean COE price across categories for each year and month\n",
    "mean_coe_by_month = monthly_coe_stats.groupby(['year', 'month'])['price'].mean().reset_index()\n",
    "\n",
    "datasets = [train_data, test_data]\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    # Merge train_data with monthly_stats\n",
    "    dataset = dataset.merge(\n",
    "        mean_coe_by_month,\n",
    "        left_on=['rent_year', 'rent_month'],\n",
    "        right_on=['year', 'month'],\n",
    "        how='left'\n",
    "    )\n",
    "    # Rename the 'price' column to 'mean_coe_price' to avoid confusion\n",
    "    dataset.rename(columns={'price': 'mean_coe_price'}, inplace=True)\n",
    "    # Drop the 'year' and 'month' columns from merged_data as they are redundant\n",
    "    dataset.drop(columns=['year', 'month'], inplace=True)\n",
    "    # Update the original datasets\n",
    "    datasets[idx] = dataset\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Statistically Encoded Features\n",
    "* group the dataframe for each categorical feature\n",
    "* compute each group's statistics and merge to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 a) Encoding w.r.t. 'floor_area_sqm' Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "cat_features = ['flat_model', 'street_name']\n",
    "\n",
    "encoder = GroupedStatsEncoder(target_column='floor_area_sqm', group_columns=cat_features)\n",
    "encoder.fit(train_data)\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    datasets[idx] = encoder.transform(dataset)\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.4 b) Encoding w.r.t. 'HDB_age' Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "\n",
    "encoder = GroupedStatsEncoder(target_column='HDB_age', group_columns=cat_features)\n",
    "encoder.fit(train_data)\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    datasets[idx] = encoder.transform(dataset)\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.5 Likelihood Encoded Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.5 a) P(rent/sqm | Categorical Feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datasets = [train_data, test_data]\n",
    "\n",
    "# compute unit price\n",
    "train_data['rent_per_sqm'] = train_data['monthly_rent'] / train_data['floor_area_sqm']\n",
    "test_data['rent_per_sqm'] = test_data['monthly_rent'] / test_data['floor_area_sqm']\n",
    "\n",
    "# Compute and merge statistics for each categorical column\n",
    "for cat_feature in urban_planning:\n",
    "    encoder = LikelihoodEncoder(cat_feature, target_feature='rent_per_sqm', alpha=1, noise_std=0.1)\n",
    "    encoder.fit(train_data)\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        dataset = encoder.transform(dataset)\n",
    "\n",
    "for cat_feature in urban_planning:\n",
    "    encoder = LikelihoodEncoder(cat_feature, target_feature='monthly_rent', alpha=1, noise_std=0.1)\n",
    "    encoder.fit(train_data)\n",
    "    for idx, dataset in enumerate(datasets):\n",
    "        dataset = encoder.transform(dataset)\n",
    "\n",
    "for idx, dataset in enumerate(datasets):\n",
    "    dataset.drop(columns=cat_features, inplace=True)\n",
    "    dataset.drop(columns=['planning_area', 'subzone', 'town'], inplace=True)\n",
    "    dataset.drop(columns='rent_per_sqm', inplace=True)\n",
    "\n",
    "train_data, test_data = datasets\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.6 Clustering as a Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.6 a) K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Fit the K-Means model on the standardized train_data\n",
    "# coords_train = train_data[['longitude', 'latitude']]\n",
    "# scaler = StandardScaler()\n",
    "# coords_train_scaled = scaler.fit_transform(coords_train)\n",
    "#\n",
    "# kmeans = KMeans(n_clusters=12, init='k-means++', random_state=0, n_init=1)\n",
    "# kmeans.fit(coords_train_scaled)\n",
    "#\n",
    "# # Assign cluster labels to train_data using the trained K-Means model\n",
    "# train_data['cluster'] = kmeans.predict(coords_train_scaled)\n",
    "#\n",
    "# # Extract and standardize the 'longitude' and 'latitude' features from test_data\n",
    "# coords_test = test_data[['longitude', 'latitude']]\n",
    "# coords_test_scaled = scaler.transform(coords_test)  # Use the same scaler object that was fit on train_data\n",
    "#\n",
    "# # Assign cluster labels to test_data using the trained K-Means model\n",
    "# test_data['cluster'] = kmeans.predict(coords_test_scaled)\n",
    "#\n",
    "# train_data, test_data = datasets\n",
    "# train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.7 Magic Feature from Freature Importance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "feature_pairs = determine_feature_crosses(train_data, 'monthly_rent', 3)\n",
    "# Now create the new crossed features in the DataFrame\n",
    "train_data = create_feature_crosses(train_data, feature_pairs)\n",
    "test_data = create_feature_crosses(test_data, feature_pairs)\n",
    "train_data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nan_counts = test_data.isna().sum()\n",
    "features_with_nan = nan_counts[nan_counts > 0]\n",
    "print(train_data.shape, test_data.shape, features_with_nan)\n",
    "\n",
    "train_data.drop(columns=['latitude', 'longitude'], inplace=True)\n",
    "test_data.drop(columns=['latitude', 'longitude'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.7 a) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2.4 a) PCA\n",
    "train_data_without_target = train_data.drop(columns=['monthly_rent'])\n",
    "\n",
    "# 1. Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_data_standardized = scaler.fit_transform(train_data_without_target)\n",
    "\n",
    "# 2. Perform PCA\n",
    "pca = PCA()\n",
    "pca_result = pca.fit(train_data_standardized)\n",
    "\n",
    "# 3. Calculate the explained variance ratio for each feature\n",
    "explained_variance_ratio = pca_result.explained_variance_ratio_\n",
    "# Calculate the cumulative sum of the explained variance ratio\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "# Find the number of components required to explain at least 90% of the variance\n",
    "num_components = np.where(cumulative_variance >= 0.90)[0][0] + 1\n",
    "\n",
    "# Create a DataFrame to display feature contributions\n",
    "feature_contributions = pd.DataFrame({\n",
    "    'Feature': train_data_without_target.columns,\n",
    "    'Contribution to Variance': explained_variance_ratio\n",
    "})\n",
    "print(feature_contributions)\n",
    "\n",
    "# most_contribute_feature\n",
    "most_contribute_feature = np.array(feature_contributions['Feature'])\n",
    "\n",
    "# Plotting the explained variance ratio\n",
    "feature_names = most_contribute_feature\n",
    "\n",
    "# Plotting the explained variance ratio with feature names\n",
    "plt.figure(figsize=(11, 5))  # Increase figure size for clarity\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(explained_variance_ratio)))  # Generate a color map\n",
    "bars = plt.bar(range(len(explained_variance_ratio)), explained_variance_ratio, alpha=0.7, align='center', color=colors)\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "\n",
    "# Add lines and text for feature names\n",
    "for idx, (bar, feature) in enumerate(zip(bars, feature_names)):\n",
    "    yval = bar.get_height()\n",
    "    line_height = 0.01\n",
    "    text_position = yval + line_height + 0.01  # Adjust text position above the line\n",
    "    line_color = 'white' if idx == 0 else 'grey'  # Set line color to white for the first index\n",
    "    text_color = 'white' if idx == 0 else 'black'  # Set text color to white for the first index\n",
    "\n",
    "    if idx < 2 and yval > 0.02:  # Assuming the second bar is also tall enough\n",
    "        text_color = 'white'\n",
    "    else:\n",
    "        text_color = 'black'\n",
    "\n",
    "    # Special adjustment for the first bar if it's too high\n",
    "    if idx in [0, 1] and yval + line_height + 0.05 > plt.ylim()[1]:\n",
    "        line_height = -0.04  # Draw line downwards\n",
    "        text_position = yval + line_height - 0.04 # Place text below the line\n",
    "\n",
    "    plt.plot([idx, idx], [yval, yval + line_height], color=line_color, lw=0.4)  # Draw a line from the bar\n",
    "    plt.text(idx, text_position, feature, ha='center', rotation=90, fontsize=14, color=text_color, fontweight='bold')  # Place the text at the end of the line\n",
    "\n",
    "plt.tight_layout()  # Adjust the layout to make room for the rotated labels\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.7 b) Top-K Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_top_k_features(dataset, k, standard=False):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the top-k most contributing features from train_data\n",
    "    and the target feature 'monthly_rent'.\n",
    "\n",
    "    Parameters:\n",
    "    - k (int): Number of top features to select.\n",
    "    - standard (Boolean): Whether to standardized or not\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with the top-k features and the target feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Top-K\n",
    "    top_k_features = list(most_contribute_feature[:k])\n",
    "    # Target\n",
    "    top_k_features.append('monthly_rent')\n",
    "\n",
    "    if standard:\n",
    "        # Separate the target feature\n",
    "        target_feature = dataset['monthly_rent'].copy()\n",
    "\n",
    "        # Remove the target feature for standardization\n",
    "        train_data_without_target = dataset.drop(columns=['monthly_rent'])\n",
    "\n",
    "        train_data_standardized = StandardScaler().fit_transform(train_data_without_target)\n",
    "\n",
    "        # Convert the array back to a DataFrame\n",
    "        train_data_standardized_df = pd.DataFrame(train_data_standardized,\n",
    "                                                  columns=train_data_without_target.columns)\n",
    "\n",
    "        # Reset indices and add the target feature back to the standardized DataFrame\n",
    "        train_data_standardized_df = train_data_standardized_df.reset_index(drop=True)\n",
    "        target_feature = target_feature.reset_index(drop=True)\n",
    "        train_data_standardized_df['monthly_rent'] = target_feature\n",
    "\n",
    "    return dataset[top_k_features].copy()\n",
    "\n",
    "# test_na_set = get_top_k_features(train_data, 20, True)\n",
    "# nan_counts = test_na_set.isna().sum()\n",
    "# features_with_nan = nan_counts[nan_counts > 0]\n",
    "# print(features_with_nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3.7 c) Variance contribution perccentage selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_var_percentage_features(dataset, percentage, standard=False):\n",
    "    # Sum up contribution to variance\n",
    "    total_variance = np.sum(feature_contributions['Contribution to Variance'])\n",
    "\n",
    "    # Calculate the cumulative contribution to variance in percentage\n",
    "    feature_contributions['Cumulative Contribution'] = feature_contributions['Contribution to Variance'].cumsum() / total_variance * 100\n",
    "\n",
    "    # Find the number of features where the cumulative contribution is just over 90%\n",
    "    k = feature_contributions[feature_contributions['Cumulative Contribution'] <= percentage].shape[0] + 1\n",
    "\n",
    "    return get_top_k_features(dataset, k, standard)\n",
    "\n",
    "# test_df = get_var_percentage_features(train_data, 95)\n",
    "# test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 3. Save and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save data into file\n",
    "train_data.to_csv('preprocessed/train_data.csv', index=0)\n",
    "test_data.to_csv('preprocessed/test_data.csv', index=0)\n",
    "np.save('preprocessed/most_contribute_feature.npy', most_contribute_feature, allow_pickle=True)\n",
    "feature_contributions.to_csv('preprocessed/feature_contributions.csv', index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load data from file\n",
    "train_data = pd.read_csv('preprocessed/train_data.csv')\n",
    "test_data = pd.read_csv('preprocessed/test_data.csv')\n",
    "most_contribute_feature = np.load('preprocessed/most_contribute_feature.npy', allow_pickle=True)\n",
    "feature_contributions = pd.read_csv('preprocessed/feature_contributions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Methods - Single Regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After EDA and data preprocessing, we can now apply data mining methods for prediction. In this section, we choose different algorithms to generate regression models, including Multiple Linear Regression, Random Forest, XGBoost and LightGBM. The general proposal is to split the preprocessed dataset into train/test data, train and validate with train data, and evaluate the optimized model performance with test data. Finally, compare the models to get a overall picture of how well these regression models perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different modelling methods may involve different data processing and feature engineering techniques. Therefore, instead of using the train_data/test_data directly, we create separate copies of the dataset for each method to generate the corresponding training/testing data for that particular method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 4.0 General Evaluate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_predict_vs_actual(y_actual, y_predict):\n",
    "    plt.scatter(y_actual, y_predict)\n",
    "    plt.xlabel(\"Actual Price\")\n",
    "    plt.ylabel(\"Predicted Price\")\n",
    "    plt.title(\"Actual vs Predicted\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get var percentage 90 features\n",
    "data_mlr = get_var_percentage_features(train_data, 90, True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data_mlr.drop(['monthly_rent'], axis=1)\n",
    "y = data_mlr['monthly_rent']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Linear Regression\n",
    "mlr = LinearRegression()\n",
    "\n",
    "# Fit the model\n",
    "mlr = mlr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = mlr.predict(X_train)\n",
    "\n",
    "# Model evaluation metrics\n",
    "print('R^2:',metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:',metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assumptions on MLR Models**\n",
    "\n",
    "There are some main assumptions on Multiple Regression Models, which are listed below:\n",
    "\n",
    "* Linearity\n",
    "* No Multi-Collinearity\n",
    "* Homoskedasticity\n",
    "* Independence of independent variable\n",
    "* Normality\n",
    "* Independence of errors\n",
    "\n",
    "We should conduct examinations on whether the assumptions above are satisfied in this context. Basic methods include using diagnostic plots for the relevant analysis. For example, we can use residual plots to check if homoskedasticity is violated or the errors' independency. Also, scatter plots of dependent versus independent variables can help us find non-linear relationships and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Linearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearity in MLR means that the relationship between the independent variables (features) and the dependent variable (target) is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, nrows=4, figsize=(25, 16))\n",
    "\n",
    "axs = axs.flatten()\n",
    "for i, k in enumerate(X_train.columns):\n",
    "    sns.regplot(y=y_train, x=X_train[k], ax=axs[i], scatter_kws={\"color\": \"teal\"}, line_kws={\"color\": \"red\"})\n",
    "plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from the plots that the relationship between the independent variables (features) and the dependent variable (target) is linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **No Multi-Collinearity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multicollinearity occurs when two or more independent variables in the MLR model are highly correlated, making it difficult to separate their individual effects on the dependent variable. It can impact the reliability of your regression model's coefficients and predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the Correlation Matrix to have a look at the multi-collinearity across all variables first. Look for high correlation coefficients (typically greater than 0.7) between pairs of variables. High correlations suggest multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Correlation Matrix\n",
    "corrMatrix = X_train.corr()\n",
    "sns.heatmap(corrMatrix, annot=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`flat_type` and `floor_area_sqm` has a correlation coefficient of 0.95, and `town` and `planning_area` has a correlation coefficient of 0.97. This may suggest multicollinearity.\n",
    "\n",
    "Note that `flat_type`, `town` and `planning_area` are  encoded and is not numeric values by nature, so we may consider other encoding methods for avoiding multi-collinearity across the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use another metrics Variance Inflation Factor (VIF) to measure how much the variance of the estimated regression coefficients is increased due to multi-collinearity. Calculate the VIF for each independent variable. High VIF values (typically greater than 10) indicate multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "vif = pd.DataFrame({\n",
    "    'variables': X_train.columns,\n",
    "    'VIF': [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n",
    "}).sort_values(by='VIF', ascending=True)\n",
    "\n",
    "print(vif.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, high VIF values (typically greater than 10) indicate multicollinearity. The form shows features \"flat_type\", \"floor_area_sqm\", \"HDB_age\", \"rent_year\", \"school_count\" and \"mall_count\" may have issues of multicollinearity, among which the VIF value of \"rent_year\" is especially high, suggesting the potential inner problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Homoskedasticity**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homoscedasticity means that the variance of the residuals (the differences between the observed values and the predicted values) is constant across all levels of the independent variables. If homoscedasticity is violated, it can lead to problems such as biased coefficient estimates and incorrect inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(40, 32))\n",
    "for i, k in enumerate(X_train.columns):\n",
    "    plt.subplot(5, 4, i + 1)\n",
    "    plt.scatter(X_train[k], y_train - y_pred_train)\n",
    "    plt.xlabel(k)\n",
    "    plt.ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there is a random scattering of points with no discernible pattern. As in a homoskedastic dataset, the points should be evenly scattered around the horizontal line at 0 (the residuals have constant variance), we can come to the conclusion that the Homoskedasticity is not violated.\n",
    "\n",
    "Also, we do not observe a funnel-shaped, fan-shaped pattern, or any other systematic change in the spread of residuals as the fitted values change, which suggests heteroskedasticity (the variance of residuals is not constant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Independence of independent variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLR assumes that independent variables are not perfectly correlated with each other (i.e., they are independent). Violations of this assumption can lead to unstable coefficient estimates and difficulties in interpreting their individual effects. We perform check of the independence, and some typical ones are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of X variables-'floor_area_sqm'\n",
    "plt.scatter(X_train['floor_area_sqm'], y_train - y_pred_train)\n",
    "plt.title(\"Floor Area Size vs Residuals\")\n",
    "plt.xlabel(\"floor_area_sqm\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of X variables-'latitude'\n",
    "# plt.scatter(X_train['latitude'], y_train - y_pred_train)\n",
    "# plt.title(\"Latitude vs Residuals\")\n",
    "# plt.xlabel(\"latitude\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence of X variables-'longitude'\n",
    "# plt.scatter(X_train['longitude'], y_train - y_pred_train)\n",
    "# plt.title(\"Longitude vs Residuals\")\n",
    "# plt.xlabel(\"latitude\")\n",
    "# plt.ylabel(\"Residuals\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots above all show that residuals do appear randomly and symmetrically distributed around zero under all conditions, which prove the independence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Normality**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLR assumes that the residuals (the differences between the observed and predicted values) are normally distributed. Deviations from normality can impact the validity of statistical inference, such as hypothesis tests and confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of residuals\n",
    "sns.distplot(y_train - y_pred_train)\n",
    "plt.title(\"Normality of Residuals\")\n",
    "plt.xlabel(\"Residuals\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the plot above that the residuals are normally distributed, which means the normality assumption is satiefied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Independence of errors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independence of errors states that the residuals (the differences between the observed values and the predicted values) should be independent of each other. Violations of this assumption can lead to incorrect parameter estimates, unreliable hypothesis tests, and inaccurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use *Durbin-Watson Test* for checking independence of errors. This statistical test checks for the presence of autocorrelation in the residuals. A Durbin-Watson statistic value close to 2 indicates no autocorrelation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.stattools import durbin_watson\n",
    "durbin_watson_statistic = durbin_watson(y_train - y_pred_train)\n",
    "print(f'Durbin-Watson Statistic: {durbin_watson_statistic}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Durbin-Watson statistic value is close to 2, which means there is no autocorrelation (i.e. the assmuption is satisfied)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate the model on test dataset\n",
    "\n",
    "After the evalution on the training dataset, as well as the checks for MLR assmuptions and corresponding model refinement, we can use the model as the final  MLR model and evaluate its performance on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = mlr.predict(X_test)\n",
    "\n",
    "# Model evaluation metrics\n",
    "print('R^2:',metrics.r2_score(y_test, y_pred_test))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_test, y_pred_test))\n",
    "print('MSE:',metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the training and testing result of the multiple linear regression model, we could find that the R^2 and RMSE is not very ideal. This might due to we use a very intuitive selection of features, and the simple structure of linear regression model could not handle the complexity of such multiple dimensions. Also we have noticed that there are inner problems of the multiple linear regression model such as the multi-collinearity, which suggests a single MLR model is not an potimal choice in the current context. But still we could base on this model to provide insights on other models, for example the selection of the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Support Vector Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get var percentage 90 features with standardization\n",
    "data_svr = get_var_percentage_features(train_data, 90, True)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data_svr.drop(['monthly_rent'], axis=1)\n",
    "y = data_svr['monthly_rent']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize Support Vector Regression with appropriate parameters\n",
    "# svr = SVR(kernel='linear', C=1.0, epsilon=0.2)\n",
    "svr = SVR(max_iter=10000)\n",
    "\n",
    "# Fit the model\n",
    "svr = svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "y_pred_train = svr.predict(X_train)\n",
    "\n",
    "# Display training set evaluation metrics\n",
    "print('R^2:',metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:',metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "# Predict on testing set\n",
    "y_pred_test = svr.predict(X_test)\n",
    "\n",
    "# Display testing set evaluation metrics\n",
    "print()\n",
    "print('Testing Metrics:')\n",
    "print('R^2:',metrics.r2_score(y_test, y_pred_test))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_test, y_pred_test))\n",
    "print('MSE:',metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get var percentage 90 features\n",
    "data_rf = get_var_percentage_features(train_data, 90)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data_rf.drop(['monthly_rent'], axis = 1)\n",
    "y = data_rf['monthly_rent']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# Define XGBoost hyperparameters\n",
    "# We use GridSearchCV as a technique to systematically search through different combinations of hyperparameters\n",
    "rf_param_grid = {\n",
    "    'max_depth': [80, 90, 100], # Maximum number of levels in each decision tree\n",
    "    'max_features': [2, 3], # Maximum number of features considered for splitting a node\n",
    "    'min_samples_leaf': [1, 3, 4, 5], # Minimum number of data points allowed in a leaf node\n",
    "    'n_estimators': [100, 300, 600], # Number of trees in the forest\n",
    "}\n",
    "\n",
    "# Initialize RandomForest regressor and GridSearchCV\n",
    "rf_reg = RandomForestRegressor(random_state = 42)\n",
    "rf_grid = GridSearchCV(estimator = rf_reg, param_grid = rf_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "rf_grid = rf_grid.fit(X_train, y_train)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print()\n",
    "print('Best hyperparameters:')\n",
    "print(rf_grid.best_params_)\n",
    "\n",
    "# Get the best RF estimator based on best parameters generated from above\n",
    "rf_best = rf_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "y_pred_train = rf_best.predict(X_train)\n",
    "\n",
    "# Display training set evaluation metrics\n",
    "print('Training Metrics:')\n",
    "print('R^2:',metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred_train))*(len(y_train)-1)/(len(y_train)-X_train.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:',metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "# Predict on testing set\n",
    "y_pred_test = rf_best.predict(X_test)\n",
    "\n",
    "# Display testing set evaluation metrics\n",
    "print()\n",
    "print('Testing Metrics:')\n",
    "print('R^2:',metrics.r2_score(y_test, y_pred_test))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_test, y_pred_test))\n",
    "print('MSE:',metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "# Display feature importances\n",
    "rf_features = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance Score': rf_best.feature_importances_\n",
    "}).sort_values(by='Importance Score', ascending=False)\n",
    "print()\n",
    "print('Top 10 important features:')\n",
    "print(rf_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict on test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get var percentage 90 features\n",
    "test_data_rf = get_var_percentage_features(test_data, 90)\n",
    "test_data_rf = test_data_rf.drop(['monthly_rent'], axis = 1)\n",
    "\n",
    "# Predict on training set\n",
    "pred_test_rf = rf_best.predict(test_data_rf)\n",
    "\n",
    "# Save to file\n",
    "pred_test_rf_df = pd.DataFrame()\n",
    "pred_test_rf_df['id'] = [i for i in range(len(test_data))]\n",
    "pred_test_rf_df['Predicted'] = pred_test_rf\n",
    "pred_test_rf_df.to_csv('predict_result_rf.csv', index=0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get var percentage 90 features\n",
    "data_gb = get_var_percentage_features(train_data, 90)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data_gb.drop(['monthly_rent'], axis=1)\n",
    "y = data_gb['monthly_rent']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define GBoost hyperparameters\n",
    "gb_param_grid = {\n",
    "    'loss': ['quantile', 'absolute_error', 'squared_error', 'huber'],\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 8, 10],\n",
    "    'min_samples_leaf': [100, 150, 250],\n",
    "    'max_features': [0.3, 0.1]\n",
    "}\n",
    "\n",
    "# Initialize GradientBoostingRegressor\n",
    "gb_reg = GradientBoostingRegressor()\n",
    "gb_grid = GridSearchCV(estimator=gb_reg, param_grid=gb_param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "gb_grid = gb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print()\n",
    "print('Best hyperparameters:')\n",
    "print(gb_grid.best_params_)\n",
    "\n",
    "# Get best estimator\n",
    "gb_best = gb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "y_pred_train = gb_best.predict(X_train)\n",
    "\n",
    "# Display training set evaluation metrics\n",
    "print('Training Metrics:')\n",
    "print('R^2:', metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:', 1 - (1 - metrics.r2_score(y_train, y_pred_train)) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1))\n",
    "print('MAE:', metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:', metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "# Predict on testing set\n",
    "y_pred_test = gb_best.predict(X_test)\n",
    "\n",
    "# Display testing set evaluation metrics\n",
    "print()\n",
    "print('Testing Metrics:')\n",
    "print('R^2:',metrics.r2_score(y_test, y_pred_test))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_test, y_pred_test))\n",
    "print('MSE:',metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "# Display feature importances\n",
    "gb_features = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance Score': gb_best.feature_importances_\n",
    "}).sort_values(by='Importance Score', ascending=False)\n",
    "print()\n",
    "print('Top 10 important features:')\n",
    "print(gb_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get var percentage 90 features\n",
    "data_xgb = get_var_percentage_features(train_data, 90)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data_xgb.drop(['monthly_rent'], axis=1)\n",
    "y = data_xgb['monthly_rent']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define XGBoost hyperparameters\n",
    "xgb_param_grid = {\n",
    "    \"learning_rate\": [0.05, 0.1, 0.2],\n",
    "    \"max_depth\": [6, 8, 9, 10],\n",
    "    \"min_child_weight\": [1, 3, 5, 7],\n",
    "    \"gamma\": [0.0, 0.1, 0.2, 0.3],\n",
    "    \"colsample_bytree\": [0.3, 0.4, 0.6, 0.8]\n",
    "}\n",
    "\n",
    "# Initialize XGBoost regressor and GridSearchCV\n",
    "xgb_reg = xgb.XGBRegressor(seed=42, objective='reg:squarederror')\n",
    "xgb_grid = GridSearchCV(estimator=xgb_reg, param_grid=xgb_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "xgb_grid = xgb_grid.fit(X_train, y_train)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print()\n",
    "print('Best hyperparameters:')\n",
    "print(xgb_grid.best_params_)\n",
    "\n",
    "# Get best estimator\n",
    "xgb_best = xgb_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "y_pred_train = xgb_best.predict(X_train)\n",
    "\n",
    "# Display training set evaluation metrics\n",
    "print('Training Metrics:')\n",
    "print('R^2:', metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:', 1 - (1 - metrics.r2_score(y_train, y_pred_train)) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1))\n",
    "print('MAE:', metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:', metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "# Predict on testing set\n",
    "y_pred_test = xgb_best.predict(X_test)\n",
    "\n",
    "# Display testing set evaluation metrics\n",
    "print()\n",
    "print('Testing Metrics:')\n",
    "print('R^2:',metrics.r2_score(y_test, y_pred_test))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_test, y_pred_test))\n",
    "print('MSE:',metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "# Display feature importances\n",
    "xgb_features = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance Score': xgb_best.feature_importances_\n",
    "}).sort_values(by='Importance Score', ascending=False)\n",
    "print()\n",
    "print('Top 10 important features:')\n",
    "print(xgb_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 LightGBM Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get var percentage 90 features\n",
    "data_lgbm = get_var_percentage_features(train_data, 90)\n",
    "\n",
    "# Split data into features and target\n",
    "X = data_lgbm.drop(['monthly_rent'], axis=1)\n",
    "y = data_lgbm['monthly_rent']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define XGBoost hyperparameters\n",
    "lgbm_param_grid = {\n",
    "    'metric': ['rmse'],\n",
    "    'max_depth': [9,10,11,12,13], \n",
    "    'bagging_fraction': [0.8, 0.9, 1],\n",
    "    'feature_fraction': [0.8, 0.9, 1],\n",
    "    'min_data_in_leaf': [20,50,80],\n",
    "    'learning_rate': [0.01,0.05,0.1,0.2]\n",
    "}\n",
    "\n",
    "# Initialize LightGBM regressor and GridSearchCV\n",
    "lgbm_reg = lgb.LGBMRegressor(seed = 42, num_iterations = 1200)\n",
    "lgbm_grid = GridSearchCV(estimator=lgbm_reg, param_grid=lgbm_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "# Fit the model\n",
    "lgbm_grid = lgbm_grid.fit(X_train, y_train)\n",
    "\n",
    "# Display best hyperparameters\n",
    "print()\n",
    "print('Best hyperparameters:')\n",
    "print(lgbm_grid.best_params_)\n",
    "\n",
    "# Get best estimator\n",
    "lgbm_best = lgbm_grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on training set\n",
    "y_pred_train = lgbm_best.predict(X_train)\n",
    "\n",
    "# Display training set evaluation metrics\n",
    "print('Training Metrics:')\n",
    "print('R^2:', metrics.r2_score(y_train, y_pred_train))\n",
    "print('Adjusted R^2:', 1 - (1 - metrics.r2_score(y_train, y_pred_train)) * (len(y_train) - 1) / (len(y_train) - X_train.shape[1] - 1))\n",
    "print('MAE:', metrics.mean_absolute_error(y_train, y_pred_train))\n",
    "print('MSE:', metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_train, y_pred_train)))\n",
    "\n",
    "# Predict on testing set\n",
    "y_pred_test = lgbm_best.predict(X_test)\n",
    "\n",
    "# Display testing set evaluation metrics\n",
    "print()\n",
    "print('Testing Metrics:')\n",
    "print('R^2:',metrics.r2_score(y_test, y_pred_test))\n",
    "print('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred_test))*(len(y_test)-1)/(len(y_test)-X_test.shape[1]-1))\n",
    "print('MAE:',metrics.mean_absolute_error(y_test, y_pred_test))\n",
    "print('MSE:',metrics.mean_squared_error(y_test, y_pred_test))\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred_test)))\n",
    "\n",
    "# Display feature importances\n",
    "lgbm_features = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance Score': lgbm_best.feature_importances_\n",
    "}).sort_values(by='Importance Score', ascending=False)\n",
    "print()\n",
    "print('Top 10 important features:')\n",
    "print(lgbm_features.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Ensemble Models - Model System Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.1 Importing Upstream Work"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('preprocessed/train_data.csv')\n",
    "test_data = pd.read_csv('preprocessed/test_data.csv')\n",
    "most_contribute_feature = np.load('preprocessed/most_contribute_feature.npy', allow_pickle=True)\n",
    "feature_contributions = pd.read_csv('preprocessed/feature_contributions.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def get_var_percentage_features(dataset, percentage, standard=False):\n",
    "    # Sum up contribution to variance\n",
    "    total_variance = np.sum(feature_contributions['Contribution to Variance'])\n",
    "\n",
    "    # Calculate the cumulative contribution to variance in percentage\n",
    "    feature_contributions['Cumulative Contribution'] = feature_contributions[\n",
    "                                                           'Contribution to Variance'].cumsum() / total_variance * 100\n",
    "\n",
    "    # Find the number of features where the cumulative contribution is just over 90%\n",
    "    k = feature_contributions[feature_contributions['Cumulative Contribution'] <= percentage].shape[0] + 1\n",
    "\n",
    "    return get_top_k_features(dataset, k, standard)\n",
    "\n",
    "\n",
    "def get_top_k_features(dataset, k, standard=False):\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with the top-k most contributing features from train_data\n",
    "    and the target feature 'monthly_rent'.\n",
    "\n",
    "    Parameters:\n",
    "    - k (int): Number of top features to select.\n",
    "    - standard (Boolean): Whether to standardized or not\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: A DataFrame with the top-k features and the target feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Top-K\n",
    "    top_k_features = list(most_contribute_feature[:k])\n",
    "    # Target\n",
    "    top_k_features.append('monthly_rent')\n",
    "\n",
    "    if standard:\n",
    "        # Separate the target feature\n",
    "        target_feature = dataset['monthly_rent'].copy()\n",
    "\n",
    "        # Remove the target feature for standardization\n",
    "        train_data_without_target = dataset.drop(columns=['monthly_rent'])\n",
    "\n",
    "        train_data_standardized = StandardScaler().fit_transform(train_data_without_target)\n",
    "\n",
    "        # Convert the array back to a DataFrame\n",
    "        train_data_standardized_df = pd.DataFrame(train_data_standardized,\n",
    "                                                  columns=train_data_without_target.columns)\n",
    "\n",
    "        # Reset indices and add the target feature back to the standardized DataFrame\n",
    "        train_data_standardized_df = train_data_standardized_df.reset_index(drop=True)\n",
    "        target_feature = target_feature.reset_index(drop=True)\n",
    "        train_data_standardized_df['monthly_rent'] = target_feature\n",
    "\n",
    "    return dataset[top_k_features].copy()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = get_var_percentage_features(train_data, 90)\n",
    "test_data = get_var_percentage_features(test_data, 90)\n",
    "train_data_rent = train_data.loc[:, ['monthly_rent']]\n",
    "train_data_rent.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import feature engineering\n",
    "feature_after_engineering = train_data.columns.tolist()\n",
    "feature_after_engineering = [i for i in train_data.columns.tolist() if i != 'monthly_rent']\n",
    "\n",
    "print([i for i in feature_after_engineering])\n",
    "print(len([i for i in feature_after_engineering]))\n",
    "train_data_ensemble = train_data.copy().drop(columns=['monthly_rent'])\n",
    "test_data_ensemble = test_data.copy().drop(columns=['monthly_rent'])\n",
    "train_data_rent = train_data.loc[:, ['monthly_rent']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_ensemble = train_data_ensemble.loc[:, feature_after_engineering].copy()\n",
    "test_data_ensemble = test_data_ensemble.loc[:, feature_after_engineering].copy()\n",
    "train_data_final = train_data_ensemble\n",
    "\n",
    "train_data_ensemble.shape, test_data_ensemble.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rmse(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "def rmsle_cross_val(X_train, y_train, n_folds, model):\n",
    "    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n",
    "    rmse = np.sqrt(-cross_val_score(model, X_train.values, y_train, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return rmse\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_ensemble.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data_ensemble.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data_rent.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.2 Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.1 The Construction of the First Layer of Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # LightGBM model\n",
    "# param_grid = {\n",
    "#     'num_leaves': [20, 30, 35, 40],\n",
    "#     'learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'max_bin': [150, 200, 250],\n",
    "#     'n_estimators': [1000, 1500, 2000],\n",
    "# }\n",
    "\n",
    "# model_lgb = lgb.LGBMRegressor(objective='regression', random_state=42)\n",
    "# grid_search = GridSearchCV(estimator=model_lgb, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "# grid_search.fit(train_data_ensemble, np.log(train_data_rent))\n",
    "# best_params = grid_search.best_params_\n",
    "# print(\"Best params：\", best_params)\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(\"Best model：\", best_model)\n",
    "# model_lgb = best_model\n",
    "\n",
    "# params：{'learning_rate': 0.05, 'max_bin': 150, 'n_estimators': 1000, 'num_leaves': 20}\n",
    "# Best model： LGBMRegressor(learning_rate=0.05, max_bin=150, n_estimators=1000, num_leaves=20,\n",
    "#               objective='regression', random_state=42)\n",
    "\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',\n",
    "                              num_leaves=20, learning_rate=0.05,\n",
    "                              max_bin=150, n_estimators=1000,\n",
    "                              random_state=42)\n",
    "score_lgb = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_lgb)\n",
    "print(\"LGB score: {:.4f}\\n\".format(score_lgb.mean()))\n",
    "\n",
    "model_lgb.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_lgb_pred = model_lgb.predict(test_data_ensemble.values)\n",
    "\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_lgb_pred), columns=['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "submission.to_csv('submission/lgb_rmsle_' + str(float(score_lgb.mean()))[:7] + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # XGB model\n",
    "# param_grid = {\n",
    "#     'learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'max_depth': [6, 8, 10],\n",
    "#     'min_child_weight': [1, 5, 10],\n",
    "#     'n_estimators': [1000, 2000, 3000],\n",
    "#     'colsample_bytree': [0.7, 0.8, 0.9],\n",
    "#     'reg_alpha': [0.1, 0.5, 1.0],\n",
    "#     'reg_lambda': [0.5, 1.0, 1.5],\n",
    "#     'subsample': [0.8, 0.9, 1.0]\n",
    "# }\n",
    "\n",
    "# model_xgb = xgb.XGBRegressor(random_state=42, nthread=-1)\n",
    "# grid_search = GridSearchCV(estimator=model_xgb, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "# grid_search.fit(train_data_ensemble, np.log(train_data_rent))\n",
    "# best_params = grid_search.best_params_\n",
    "# print(\"Best params：\", best_params)\n",
    "# best_model = grid_search.best_estimator_\n",
    "# print(\"Best model：\", best_model)\n",
    "# model_xgb = best_model\n",
    "\n",
    "model_xgb = xgb.XGBRegressor(learning_rate=0.01, gamma=0,\n",
    "                             max_depth=5, min_child_weight=1,\n",
    "                             n_estimators=6000, colsample_bytree=0.6,\n",
    "                             reg_alpha=0.5, reg_lambda=1.0,\n",
    "                             subsample=0.7, random_state=42, nthread=-1)\n",
    "score_xgb = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_xgb)\n",
    "print(\"XGB rmsle mean: {:.4f}\\n\".format(score_xgb.mean()))\n",
    "\n",
    "model_xgb.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_xgb_pred = model_xgb.predict(test_data_ensemble.values)\n",
    "\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_xgb_pred), columns=['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "submission.to_csv('submission/xgb_rmsle_' + str(float(score_xgb.mean()))[:7] + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # GBoost model\n",
    "model_GBoost = GradientBoostingRegressor(n_estimators=7000, learning_rate=0.1,\n",
    "                                         max_depth=5, max_features='sqrt',\n",
    "                                         min_samples_leaf=15, min_samples_split=10,\n",
    "                                         loss='huber', random_state=42)\n",
    "score_GBoost = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_GBoost)\n",
    "print(\"GradientBoosting rmsle mean: {:.4f}\\n\".format(score_GBoost.mean()))\n",
    "\n",
    "model_GBoost.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_gb_pred = model_GBoost.predict(test_data_ensemble.values)\n",
    "\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_gb_pred), columns=['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "submission.to_csv('submission/gb_rmsle_' + str(float(score_GBoost.mean()))[:7] + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Random Forest model\n",
    "# train_data_final = train_data_ensemble[feature_after_engineering].copy()\n",
    "# model_rf = RandomForestRegressor(oob_score=True, n_jobs=-1)\n",
    "# params ={\n",
    "#     'n_estimators': [1200, 1250,1280],\n",
    "#     'min_samples_leaf': [1],\n",
    "#     'max_features': [0.5],\n",
    "#     'max_depth': [35],\n",
    "#     'min_samples_split': [2]\n",
    "# }\n",
    "# best_score = 0\n",
    "# for g in ParameterGrid(params):\n",
    "#     model_rf.set_params(**g)\n",
    "#     model_rf.fit(train_data_final, train_data_rent)\n",
    "#     if model_rf.oob_score_ > best_score:\n",
    "#         score_rf = rmsle_cross_val(train_data_final, train_data_rent, 5, model_rf)\n",
    "#         best_score = model_rf.oob_score_\n",
    "#         best_grid = g\n",
    "#         print('best oob:', best_score, best_grid, 'score:', score_rf.mean())\n",
    "\n",
    "model_rf = RandomForestRegressor(n_jobs=-1, n_estimators=1250, oob_score=True, max_depth=35, min_samples_leaf=1,\n",
    "                                 min_samples_split=2, max_features=0.5)\n",
    "score_rf = rmsle_cross_val(train_data_final, train_data_rent, 5, model_rf)\n",
    "print(\"Random Forest rmsle mean: {:.4f}\\n\".format(score_rf.mean()))\n",
    "\n",
    "model_rf.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_rf_pred = model_rf.predict(test_data_ensemble[feature_after_engineering])\n",
    "\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_rf_pred), columns=['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "submission.to_csv('submission/rf_rmsle_' + str(float(score_rf.mean()))[:7] + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.2 The Construction of the Second Layer - Stacking"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # stacking model\n",
    "model_stack = StackingCVRegressor(regressors=(model_xgb, model_lgb),\n",
    "                                  meta_regressor=model_rf,\n",
    "                                  use_features_in_secondary=True)\n",
    "score_stack = rmsle_cross_val(train_data_ensemble, np.log(train_data_rent), 5, model_stack)\n",
    "print(\"Stacked rmsle mean: {:.4f}\\n\".format(score_stack.mean()))\n",
    "\n",
    "model_stack.fit(train_data_ensemble.values, train_data_rent.values)\n",
    "model_stack_pred = model_stack.predict(test_data_ensemble.values)\n",
    "\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_stack_pred), columns=['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "submission.to_csv('submission/stacked_rmsle_' + str(float(score_stack.mean()))[:7] + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2.3 The Construction of the Third Layer - Blending"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# weight calculator\n",
    "denominator = (1 - score_lgb.mean()) + (1 - score_xgb.mean()) + (1 - score_rf.mean()) + (1 - score_stack.mean())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # Blend models/Weighting\n",
    "def blended_predictions(train_x):\n",
    "    return (((1 - score_lgb.mean()) / denominator * model_xgb.predict(train_x)) + \\\n",
    "            ((1 - score_xgb.mean()) / denominator * model_lgb.predict(train_x)) + \\\n",
    "            ((1 - score_rf.mean()) / denominator * model_rf.predict(train_x)) + \\\n",
    "            ((1 - score_stack.mean()) / denominator * model_stack.predict(train_x)))\n",
    "\n",
    "\n",
    "model_blend_pred = blended_predictions(train_data_ensemble.values)\n",
    "score_blend = rmse(np.log(train_data_rent.values), np.log(model_blend_pred))\n",
    "print(score_blend)\n",
    "model_blended_pred = blended_predictions(test_data_ensemble.values)\n",
    "\n",
    "df_id = pd.DataFrame([id for id in range(test_data.shape[0])], columns=['Id'])\n",
    "df_pred = pd.DataFrame(pd.Series(model_blended_pred), columns=['Predicted'])\n",
    "submission = pd.concat([df_id['Id'], df_pred['Predicted']], axis=1)\n",
    "submission.to_csv('submission/blended_rmsle_' + str(score_blend)[:7] + '.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
